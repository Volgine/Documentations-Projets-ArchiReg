# ğŸ”§ ARCHITECTURE WORKERLOCAL

**Date** : 15 octobre 2025  
**Version** : 3.2  
**Type** : CLI Python (Windows/Linux compatible)  
**RÃ´le** : Parser documents + GÃ©nÃ©rer embeddings **GLOBAUX**

---

## ğŸ¯ RESPONSABILITÃ‰S

### **CE QU'IL FAIT** âœ…

```
1. Lit files_queue (status: pending)
   â†“ TÃ©lÃ©charge JSON bucket
2. Parse document lÃ©gal
   â†“ Extraction texte + mÃ©tadonnÃ©es
3. GÃ©nÃ¨re embedding GLOBAL (GGUF)
   â†“ Document entier â†’ 1 vecteur 768 dims
4. INSERT dans documents table
   â†“ + Embedding pgvector
5. UPDATE files_queue (status: processed)
   â†“ INSERT parsed_files (tracking)
```

**Workflow** :
1. **SELECT** : RÃ©cupÃ¨re fichiers pending depuis `files_queue`
2. **DOWNLOAD** : TÃ©lÃ©charge JSON depuis bucket Supabase
3. **PARSE** : Extrait titre, contenu, mÃ©tadonnÃ©es
4. **EMBED** : GÃ©nÃ¨re embedding GLOBAL (document complet)
5. **INSERT** : Stocke dans `documents` + `parsed_files`
6. **UPDATE** : Marque `files_queue` comme processed

### **CE QU'IL NE FAIT PAS** âŒ

- âŒ Collecte donnÃ©es (Micro-service LÃ©gifrance)
- âŒ Chunking granulaire (WorkerLocal Chunk)
- âŒ Chat LLM (Backend)
- âŒ RAG search (Backend)

---

## ğŸ“Š ARCHITECTURE TECHNIQUE

### **ModÃ¨le GGUF Local**

**Fichier** : `solon-embeddings-large-0.1-Q8_0.gguf`

**CaractÃ©ristiques** :
- **Dimensions** : 768
- **Quantization** : Q8_0 (8-bit)
- **Taille** : ~500 MB
- **Source** : Bucket Supabase `ai-models`
- **Compilation** : Source without AVX2/FMA (compatibility)

---

## ğŸ“ STRUCTURE FICHIERS

```
WorkerLocal/
â”œâ”€â”€ cli.py                      # Point d'entrÃ©e CLI
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py            # Configuration env
â”œâ”€â”€ db/
â”‚   â””â”€â”€ supabase_client.py     # Client asyncpg
â”œâ”€â”€ embedding/
â”‚   â””â”€â”€ gguf_service.py        # Service embeddings GGUF
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ supabase_storage.py    # Download bucket
â”œâ”€â”€ worker/
â”‚   â”œâ”€â”€ parser.py              # Parsing JSON lÃ©gal
â”‚   â””â”€â”€ processor.py           # Workflow complet
â”œâ”€â”€ launch/
â”‚   â”œâ”€â”€ worker_1.bat           # Launcher Worker 1
â”‚   â”œâ”€â”€ worker_2.bat           # Launcher Worker 2
â”‚   â””â”€â”€ worker_3.bat           # Launcher Worker 3
â”œâ”€â”€ cache/                      # Cache modÃ¨le GGUF
â”œâ”€â”€ requirements.txt            # DÃ©pendances
â””â”€â”€ README.md                   # Documentation
```

---

## ğŸ”§ COMPOSANTS PRINCIPAUX

### **1. CLI** (`cli.py`)

```python
@click.command()
@click.option('--worker-id', required=True, help='Worker ID (1, 2, 3)')
@click.option('--batch-size', default=100, help='Files per batch')
@click.option('--max-iterations', default=None, help='Max iterations')
def main(worker_id: int, batch_size: int, max_iterations: int):
    """
    Lance worker parsing + embedding GLOBAL
    """
    processor = DocumentProcessor(
        worker_id=worker_id,
        batch_size=batch_size
    )
    
    asyncio.run(processor.run(max_iterations=max_iterations))
```

**Lancement** :
```bash
python cli.py --worker-id 1 --batch-size 100
```

---

### **2. DocumentProcessor** (`worker/processor.py`)

```python
class DocumentProcessor:
    def __init__(self, worker_id: int, batch_size: int):
        self.worker_id = worker_id
        self.batch_size = batch_size
        self.db_client = SupabaseDBClient()
        self.storage_client = SupabaseStorageClient()
        self.embedding_service = GGUFEmbeddingService()
        self.parser = LegalDocumentParser()
    
    async def run(self, max_iterations: int = None):
        """
        Workflow principal : Boucle infinie de traitement
        """
        await self.db_client.initialize()
        await self.embedding_service.initialize()
        
        iteration = 0
        
        while max_iterations is None or iteration < max_iterations:
            # 1. RÃ©cupÃ©rer batch files_queue
            files = await self.db_client.get_pending_files(
                limit=self.batch_size,
                worker_id=self.worker_id
            )
            
            if not files:
                logger.info("â¸ï¸ No pending files, waiting...", worker_id=self.worker_id)
                await asyncio.sleep(60)
                continue
            
            # 2. Traiter chaque fichier
            for file in files:
                try:
                    await self.process_file(file)
                except Exception as e:
                    logger.error("âŒ Error processing file", file_path=file['path'], error=str(e))
            
            iteration += 1
    
    async def process_file(self, file: dict):
        """
        Traite un fichier complet
        """
        # 1. Download JSON bucket
        content = await self.storage_client.download(
            bucket=file['bucket_name'],
            path=file['file_path']
        )
        
        # 2. Parse document
        document = self.parser.parse(content)
        
        # 3. GÃ©nÃ¨re embedding GLOBAL
        embedding = await self.embedding_service.generate(document['content'])
        
        # 4. INSERT documents + parsed_files
        await self.db_client.insert_document(
            title=document['title'],
            content=document['content'],
            metadata=document['metadata'],
            embedding=embedding,
            source_file=file['file_path']
        )
        
        # 5. UPDATE files_queue status
        await self.db_client.update_file_status(
            file_id=file['id'],
            status='processed',
            worker_id=self.worker_id
        )
        
        logger.info("âœ… File processed", file_path=file['file_path'], worker_id=self.worker_id)
```

---

### **3. GGUFEmbeddingService** (`embedding/gguf_service.py`)

```python
class GGUFEmbeddingService:
    def __init__(self):
        self.model_name = "solon-embeddings-large-0.1-Q8_0.gguf"
        self.llm = None
    
    async def initialize(self):
        """
        TÃ©lÃ©charge modÃ¨le depuis Supabase + init llama-cpp
        """
        if self.llm is not None:
            return
        
        # Cache local
        cache_dir = Path("cache/models")
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = cache_dir / self.model_name
        
        if not model_path.exists():
            # TÃ©lÃ©charger depuis bucket Supabase
            logger.info("ğŸ“¥ Downloading GGUF model...")
            content = await supabase_storage.download("ai-models", self.model_name)
            
            with open(model_path, "wb") as f:
                f.write(content)
            
            logger.info("âœ… Model downloaded")
        
        # Init llama-cpp
        self.llm = Llama(
            model_path=str(model_path),
            embedding=True,
            n_ctx=2048,         # Context window (document entier)
            n_threads=4,
            verbose=False
        )
        
        logger.info("âœ… GGUF service initialized")
    
    async def generate(self, text: str) -> List[float]:
        """
        GÃ©nÃ¨re embedding 768 dimensions
        """
        await self.initialize()
        
        embedding = self.llm.embed(text)
        
        if len(embedding) != 768:
            raise ValueError(f"Expected 768 dims, got {len(embedding)}")
        
        return embedding
```

---

### **4. LegalDocumentParser** (`worker/parser.py`)

```python
class LegalDocumentParser:
    def parse(self, json_content: dict) -> dict:
        """
        Parse JSON document LÃ©gifrance
        Extrait titre, contenu, mÃ©tadonnÃ©es
        """
        # GÃ©rer structure API PISTE
        if 'article' in json_content:
            article = json_content['article']
        else:
            article = json_content
        
        # Extraction donnÃ©es
        title = self._extract_title(article)
        content = self._extract_content(article)
        metadata = self._extract_metadata(article)
        
        return {
            "title": title,
            "content": content,
            "metadata": metadata
        }
    
    def _extract_content(self, article: dict) -> str:
        """
        Extrait texte depuis texteHtml
        Nettoie HTML
        """
        texte_html = article.get('texteHtml', '')
        
        if not texte_html:
            return ""
        
        # Nettoyage HTML
        texte_clean = re.sub(r'<[^>]+>', ' ', texte_html)
        texte_clean = texte_clean.replace('&nbsp;', ' ').replace('&amp;', '&')
        texte_clean = re.sub(r'\s+', ' ', texte_clean).strip()
        
        return texte_clean
    
    def _extract_metadata(self, article: dict) -> dict:
        """
        Extrait mÃ©tadonnÃ©es pertinentes
        """
        return {
            "article_id": article.get('id'),
            "num": article.get('num'),
            "nature": article.get('nature'),
            "etat": article.get('etat'),
            "dateDebut": article.get('dateDebut'),
            "datePubli": article.get('datePubli'),
            "nota": article.get('nota', '')
        }
```

---

## ğŸ”„ WORKFLOW COMPLET

```mermaid
sequenceDiagram
    participant WL as WorkerLocal
    participant FQ as files_queue
    participant BKT as Bucket
    participant GGUF as GGUF Service
    participant DB as documents table
    
    loop Boucle infinie
        WL->>FQ: SELECT * WHERE status=pending LIMIT 100
        FQ-->>WL: 100 fichiers
        
        loop Pour chaque fichier
            WL->>BKT: Download JSON
            BKT-->>WL: JSON content
            
            WL->>WL: Parse document
            
            WL->>GGUF: generate(content)
            GGUF-->>WL: embedding[768]
            
            WL->>DB: INSERT documents + embedding
            WL->>FQ: UPDATE status=processed
        end
        
        alt Pas de fichiers pending
            WL->>WL: Sleep 60s
        end
    end
```

---

## âš™ï¸ CONFIGURATION

### **Variables d'environnement**

**Fichier** : `.env`

```bash
# Supabase
SUPABASE_URL=https://joozqsjbcwrqyeqepnev.supabase.co
SUPABASE_SERVICE_ROLE_KEY=xxx
DATABASE_URL=postgresql://postgres.joozqsjbcwrqyeqepnev:***@aws-0-eu-central-1.pooler.supabase.com:5432/postgres

# GGUF Model
GGUF_MODEL_NAME=solon-embeddings-large-0.1-Q8_0.gguf
GGUF_BUCKET_NAME=ai-models
EMBEDDING_DIM=768

# Worker Config
BATCH_SIZE=100
MAX_RETRIES=3
```

---

### **Launchers Windows**

**Fichier** : `launch/worker_1.bat`

```batch
@echo off
cd /d "%~dp0.."
call venv\Scripts\activate
python cli.py --worker-id 1 --batch-size 100
pause
```

**Usage** :
```batch
# Lancer Worker 1
launch\worker_1.bat

# Lancer Worker 2
launch\worker_2.bat

# Lancer Worker 3
launch\worker_3.bat
```

---

## ğŸ“Š PERFORMANCE

### **Stats RÃ©elles**

**Configuration** : 3 Workers simultanÃ©s

| MÃ©trique | Valeur | Notes |
|----------|--------|-------|
| **Vitesse** | ~12.5 fichiers/s/worker | 37.5/s total (3 workers) |
| **Processed** | 312,000 documents | âœ… TerminÃ© |
| **Latence embedding** | ~50-100ms | GGUF CPU |
| **Taux erreurs** | <0.03% | TrÃ¨s fiable |

---

## ğŸ”§ FIX CRITIQUE

### **Embeddings Incompatibles** (16-FIX-EMBEDDINGS-INCOMPATIBLES.md)

**ProblÃ¨me** : Workers (Windows AVX2) â‰  Backend (Linux no-AVX2)

**Solution** :
```bash
# Forcer compilation source sans AVX2/FMA
pip uninstall llama-cpp-python
pip install --no-binary=llama-cpp-python llama-cpp-python
```

**RÃ©sultat** :
- âœ… Embeddings identiques bit-perfect
- âœ… RAG trouve documents (0 â†’ 312k)
- âœ… Backend + Workers alignÃ©s

---

## ğŸ¯ RÃ©sumÃ©

**WorkerLocal optimisÃ©** :
- âœ… CLI Python multi-workers
- âœ… Parsing JSON LÃ©gifrance
- âœ… Embeddings GLOBAUX GGUF (768 dims)
- âœ… 312k documents traitÃ©s
- âœ… Vitesse : 37.5 fichiers/s (3 workers)
- âœ… Taux erreur <0.03%
- âœ… Compatible Windows/Linux

**Parsing ultra-performant !** ğŸš€

