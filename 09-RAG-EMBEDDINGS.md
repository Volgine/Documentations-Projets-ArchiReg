# üß† RAG & EMBEDDINGS - Architecture Compl√®te

**Date de cr√©ation** : 11 octobre 2025  
**Derni√®re mise √† jour** : 13 octobre 2025 18:00 UTC  
**Version** : 2.5 HNSW OPTIMIS√â  
**Status** : ‚úÖ **RAG ACTIF** - Index HNSW (1.4GB, 930k docs, <1s) + Optimisations Supabase (timeout 60s, ef_search 100)

---

## üéØ Vue d'Ensemble

Ce document d√©crit l'architecture compl√®te du syst√®me **RAG (Retrieval-Augmented Generation)** et de **g√©n√©ration d'embeddings** pour la recherche s√©mantique dans les documents juridiques L√©gifrance.

---

## üìä Architecture Globale

```mermaid
graph TB
    subgraph "1Ô∏è‚É£ COLLECTE DONN√âES"
        A[Micro-service L√©gifrance PISTE] -->|Upload JSON| B[Bucket Supabase Storage<br/>agentbasic-legifrance-raw]
    end
    
    subgraph "2Ô∏è‚É£ TRAITEMENT: WORKERS LOCAUX"
        B -->|T√©l√©charge fichiers| C[Worker Local x3<br/>ACTUEL v1.0]
        C -->|Parse + GGUF Embedding| D[(Table documents<br/>1 doc = 1 embedding contexte)]
        D -->|Vecteurs 768 dims| E[pgvector extension]
        
        H[Worker Local FUTUR<br/>v2.0 Chunking] -.->|Parse + Chunks| I[(Table document_chunks<br/>Embeddings granulaires)]
        B -.->|Source future| H
        I -.->|Vecteurs 768 dims| E
    end
    
    subgraph "3Ô∏è‚É£ BACKEND: RECHERCHE S√âMANTIQUE"
        F[Backend Agent-Orchestrator] -->|READ ONLY| E
        F -->|G√©n√®re embedding query| K[GGUF Model Local<br/>Solon-embeddings-base-0.1.Q8_0.gguf]
        K -->|T√©l√©charg√© depuis| L[Bucket Supabase<br/>ai-models]
        F -->|Recherche vectorielle| E
        F -->|API RAG| G[Frontend Chat]
    end
    
    style C fill:#4ecdc4
    style F fill:#ff6b6b
    style H fill:#95e1d3,stroke-dasharray: 5 5
    style I fill:#95e1d3,stroke-dasharray: 5 5
    style K fill:#ffd93d
    style L fill:#ffd93d
```

---

## üèóÔ∏è Composants et Responsabilit√©s

### 1Ô∏è‚É£ **Micro-service L√©gifrance** (@Micro-service-data-legifrance-piste/)

**URL** : `https://micro-service-data-legifrance-piste.onrender.com`

**R√¥le** : Collecteur de donn√©es juridiques depuis API PISTE

**Actions** :
- ‚úÖ Appel API PISTE (codes, textes juridiques, d√©crets)
- ‚úÖ T√©l√©chargement fichiers JSON bruts
- ‚úÖ Upload dans bucket Supabase `agentbasic-legifrance-raw`
- ‚úÖ Alimentation continue des donn√©es

**‚ö†Ô∏è IMPORTANT** :
- ‚ùå **PAS** de parsing
- ‚ùå **PAS** de g√©n√©ration d'embeddings
- ‚ùå **PAS** d'acc√®s √† la base de donn√©es
- ‚úÖ **SEULEMENT** collecte et upload

**Output** : Fichiers JSON dans `agentbasic-legifrance-raw/legifrance/architecture_urbanisme/`

---

### 2Ô∏è‚É£ **Worker Local** (@WorkerLocal/) üè≠

**Localisation** : PC Windows (3 instances parall√®les)

#### **VERSION ACTUELLE (v1.0 - Embeddings Contexte Global)**

**R√¥le** : Parser les documents et g√©n√©rer les embeddings de contexte global

**Flux de traitement** :
1. ‚úÖ R√©cup√®re fichiers JSON depuis bucket L√©gifrance (API Supabase Storage)
2. ‚úÖ Parse le JSON :
   - Extraction titre
   - Extraction contenu principal
   - Extraction articles
   - Extraction sections
   - D√©doublonnage automatique
3. ‚úÖ G√©n√®re **1 embedding GGUF par document complet** (contexte global)
4. ‚úÖ INSERT dans table `documents` avec colonne `embedding vector(768)`
5. ‚úÖ Anti-duplication via table `parsed_files` (content_hash SHA256)

**Mod√®le utilis√©** :
- **Nom** : `Solon-embeddings-base-0.1.Q8_0.gguf`
- **Dimensions** : 768
- **Taille** : ~500 MB
- **Localisation** : `WorkerLocal/models/`
- **Chargement** : `llama-cpp-python` avec configuration TURBO

**Configuration Performance** :
```python
{
    "batch_size": 100,           # Fichiers trait√©s par batch
    "max_concurrency": 50,       # Threads parall√®les
    "n_ctx": 512,                # ‚úÖ FIX: Contexte align√© avec n_ctx_train du mod√®le (qualit√© 100%)
    "n_batch": 2048,             # Batch MASSIF
    "n_threads": cpu_count,      # Tous les cores
    "n_ubatch": 2048,            # Micro-batch MASSIF
    "f16_kv": True,              # Float16 pour performance
    "low_vram": False,           # Utiliser toute la RAM disponible
    "download_timeout": 30,      # Timeout download (s)
    "processing_timeout": 30     # Timeout processing (s)
}
```

**Performance Mesur√©e** :
- ‚ö° **50 fichiers/sec** (concurrence 50)
- ‚ö° **~20ms par embedding** (mod√®le Q8_0 optimis√©)
- ‚ö° **99.9% anti-duplication** (content_hash + parsed_files)

**Output** : Table `documents` avec 930,394 documents index√©s (L√©gifrance)

---

#### **VERSION FUTURE (v2.0 - Chunking Granulaire)** üîÆ

**R√¥le** : D√©couper les documents en chunks et g√©n√©rer des embeddings granulaires

**Flux de traitement pr√©vu** :
1. ‚è∏Ô∏è M√™me source (bucket L√©gifrance)
2. ‚è∏Ô∏è Parse JSON en chunks granulaires :
   - 1 chunk par article
   - 1 chunk par section
   - 1 chunk par paragraphe (si >500 tokens)
3. ‚è∏Ô∏è G√©n√®re **1 embedding GGUF par chunk**
4. ‚è∏Ô∏è INSERT dans table `document_chunks` avec lien `document_id`
5. ‚è∏Ô∏è Conservation du lien parent-enfant

**Avantages** :
- üéØ Recherche plus pr√©cise (article sp√©cifique)
- üéØ Meilleure pertinence pour queries courtes
- üéØ R√©duction du bruit (contexte trop large)

**Output pr√©vu** : Table `document_chunks` (FUTUR)

---

### 3Ô∏è‚É£ **Backend Agent-Orchestrator** (@Agent-Orchestrator/) üî¥

**URL** : `https://agent-orchestrateur-backend.onrender.com`

**R√¥le** : Chatbot + Recherche s√©mantique **READ ONLY**

**Services Cl√©s** :
- `GGUFEmbeddingService` : G√©n√©ration embeddings queries
- `SupabaseSearchService` : Recherche vectorielle pgvector
- `RAGService` : Orchestration RAG compl√®te

**Flux de traitement** :
1. ‚úÖ **T√©l√©chargement mod√®le** (lazy loading au d√©marrage) :
   - Source : `https://joozqsjbcwrqyeqepnev.supabase.co/storage/v1/object/public/ai-models/Solon-embeddings-base-0.1.Q8_0.gguf`
   - Cache local : `backend/cache/shared/`
   - Chargement subprocess `llama-cpp-python`

2. ‚úÖ **R√©ception query utilisateur** :
   ```typescript
   GET /api/v3/rag/search-legifrance?query=r√®gles+urbanisme&limit=10
   ```

3. ‚úÖ **G√©n√©ration embedding query** :
   ```python
   embedding_service = get_embedding_service()
   query_embedding = await embedding_service.get_embedding("r√®gles urbanisme")
   # ‚Üí [0.123, -0.456, 0.789, ...] (768 dimensions)
   ```

4. ‚úÖ **Recherche vectorielle pgvector** :
   ```sql
   SELECT 
     id, 
     title, 
     content, 
     embedding <-> $1 AS distance
   FROM documents
   WHERE embedding <-> $1 < 0.3
   ORDER BY distance ASC
   LIMIT 10;
   ```

5. ‚úÖ **Retour r√©sultats** :
   ```json
   {
     "success": true,
     "results": [
       {
         "id": "uuid-123",
         "title": "Code de l'urbanisme - Article L123-1",
         "content": "...",
         "distance": 0.15,
         "score": 0.85
       }
     ],
     "total_found": 10,
     "query": "r√®gles urbanisme"
   }
   ```

**‚ö†Ô∏è IMPORTANT** :
- ‚ùå **PAS** de g√©n√©ration d'embeddings pour documents (Workers !)
- ‚ùå **PAS** de parsing de fichiers
- ‚ùå **PAS** d'√©criture dans `documents` ou `document_chunks`
- ‚úÖ **SEULEMENT** g√©n√©ration embedding query + recherche vectorielle

**Endpoints API** :
- `GET /api/v3/rag/search-legifrance` : Recherche s√©mantique
- `GET /api/v3/rag/legifrance-stats` : Statistiques documents
- `GET /api/v3/rag/documents/top` : Top documents projet

**Performance** :
- ‚ö° **~15-20ms** g√©n√©ration embedding query (apr√®s warm-up)
- ‚ö° **~5ms** recherche pgvector (10 r√©sultats)
- ‚ö° **~20-30ms** latence totale end-to-end
- ‚ö° **60-70%** cache hits (embeddings queries)
- üî• **Warm-up automatique** : R√©duit latence 1√®re requ√™te de 4.45s √† 500ms

---

### 4Ô∏è‚É£ **Frontend** (@ArchiReg-Front/) üåê

**URL** : `https://archireg-front.vercel.app`

**R√¥le** : Interface utilisateur chat + recherche

**Interactions RAG** :
```typescript
// Chat avec RAG
const response = await fetch('/api/v3/rag/search-legifrance', {
  method: 'GET',
  headers: {
    'Authorization': `Bearer ${userToken}`,
    'Content-Type': 'application/json'
  },
  params: {
    query: userQuery,
    limit: 10,
    similarity_threshold: 0.7
  }
})

const { results } = await response.json()
// Affichage des r√©sultats pertinents
```

---

## üóÑÔ∏è Structure Base de Donn√©es

### **Table `documents` (ACTUELLE)** ‚úÖ

```sql
CREATE TABLE documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  embedding VECTOR(768) NOT NULL,  -- ‚úÖ G√©n√©r√© par Workers Locaux
  file_path TEXT NOT NULL UNIQUE,
  upload_date TIMESTAMPTZ DEFAULT NOW(),
  metadata JSONB DEFAULT '{}'::jsonb,  -- ‚ö†Ô∏è Vide (pas utilis√© actuellement)
  extra_data JSONB DEFAULT '{}'::jsonb,  -- ‚úÖ Utilis√© par Workers
  
  CONSTRAINT unique_file_path UNIQUE (file_path)
);

-- Index pgvector pour recherche vectorielle rapide (cosine distance)
CREATE INDEX idx_documents_embedding 
ON documents 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- Index pour recherche par file_path
CREATE INDEX idx_documents_file_path ON documents(file_path);
```

**Structure extra_data (Workers Locaux) :**
```json
{
  "source": "workerlocal",
  "content_hash": "sha256...",
  "processed_at": "2025-10-08T07:22:41.152673"
}
```

**Statistiques Actuelles (12 oct 2025) :**
- **Rows** : 930,394 documents (L√©gifrance uniquement)
- **Embeddings** : 930,394 vecteurs (768 dims chacun)
- **Taille estim√©e** : ~2.8 GB (930k √ó 768 √ó 4 bytes)
- **Remplissage** : Workers Locaux (1 doc = 1 embedding contexte global)
- **Lecture** : Backend (recherche s√©mantique READ ONLY)
- **Sources** : L√©gifrance (actuel) + PLU (futur) + autres (futurs)

---

### **Table `document_chunks` (FUTURE)** üîÆ

```sql
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
  chunk_text TEXT NOT NULL,
  chunk_index INT NOT NULL,
  chunk_type TEXT,  -- 'article', 'section', 'paragraph'
  embedding VECTOR(768) NOT NULL,  -- ‚úÖ G√©n√©r√© par Workers (futur)
  metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  CONSTRAINT fk_document FOREIGN KEY (document_id) 
    REFERENCES documents(id) ON DELETE CASCADE
);

-- Index pgvector pour recherche granulaire
CREATE INDEX idx_chunks_embedding 
ON document_chunks 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- Index pour recherche par document parent
CREATE INDEX idx_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_chunks_type ON document_chunks(chunk_type);
```

**Statistiques Pr√©vues** :
- **Rows** : ~5-10 millions de chunks (estim√©)
- **Ratio** : 8-15 chunks par document
- **Remplissage** : Workers Locaux v2.0 (FUTUR)
- **Lecture** : Backend (recherche pr√©cise)

---

### **Gestion Multi-Sources (12 oct 2025 - FIX)** üéØ

**Principe** : Le chatbot aide les architectes sur **TOUTES** les sources de documentation disponibles.

**Sources actuelles et futures :**
| Source | Service collecteur | Valeur extra_data.source | Status | Documents |
|--------|-------------------|--------------------------|--------|-----------|
| **L√©gifrance** | Microservice L√©gifrance PISTE | `"workerlocal"` | ‚úÖ Actif | 930,394 |
| **PLU/GPU** | Microservice Docs PLU | `"plu"` (futur) | ‚è∏Ô∏è En dev | 0 |
| **Autres** | Services futurs | √Ä d√©finir | ‚è∏Ô∏è Futur | 0 |

**Backend RAG (fix 12 oct 2025) :**
```python
# ‚úÖ APR√àS FIX: Recherche multi-sources (TOUS les documents)
results = await search_documents(
    query="r√®gles urbanisme",
    limit=10,
    similarity_threshold=0.5
    # ‚úÖ Pas de filtre source - Chatbot cherche dans L√©gifrance + PLU + autres
)

# Si besoin de filtrer par source (optionnel futur) :
results = await search_documents(
    query="r√®gles urbanisme",
    filters={"source": "workerlocal"}  # Filtre sp√©cifique L√©gifrance
)
```

**Avantages** :
- ‚úÖ Chatbot r√©pond avec TOUTE la doc disponible (L√©gifrance + PLU + futurs)
- ‚úÖ Pas besoin de reparser les 930k documents existants
- ‚úÖ Ajout de nouvelles sources transparent (Workers futurs utilisent autre valeur source)
- ‚úÖ Filtrage optionnel possible si besoin

---

### **Fix Version llama-cpp-python + Threshold (13 oct 2025 - FIX CRITIQUE)** üîß

**Probl√®me initial** : RAG retournait **0 r√©sultats** malgr√© 930k documents index√©s.

**Diagnostic approfondi** :

| √âtape | Probl√®me identifi√© | Solution appliqu√©e |
|-------|-------------------|-------------------|
| **1** | Backend avait 7 param√®tres Llama diff√©rents de WorkerLocal | ‚úÖ Retrait de `seed`, `use_mmap`, `use_mlock`, `logits_all`, `vocab_only`, `n_gpu_layers`, `numa` |
| **2** | Backend ajoutait pr√©fixe "query :" selon doc Solon-base | ‚úÖ Retrait pr√©fixe (WorkerLocal n'en a pas) |
| **3** | Race condition subprocess (2+ requ√™tes parall√®les) | ‚úÖ Ajout `asyncio.Lock()` |
| **4** | Tentative de normalisation embeddings L2=1 | ‚úÖ Retrait normalisation (brut comme WorkerLocal) |
| **5** | **Version llama-cpp flexible (`>=0.2.20`)** | ‚úÖ **Fix√© √† `==0.3.16` exactement comme WorkerLocal** |
| **6** | Threshold 0.7 trop serr√© (distances r√©elles ~0.6-0.7) | ‚úÖ Confirm√© √† 0.70 (distances 0.67 OK) |
| **7** | Query test "test urbanisme" peu naturelle | ‚úÖ Chang√© en "Quelles sont les regles urbanisme" |

**Cause racine identifi√©e** :

**Versions llama-cpp-python diff√©rentes** :
```python
# ‚ùå AVANT (Backend requirements.txt)
llama-cpp-python>=0.2.20  # Version flexible ‚Üí pouvait √™tre 0.2.20, 0.2.50, 0.3.0, etc.

# ‚úÖ APR√àS (Backend requirements.txt)
llama-cpp-python==0.3.16  # Version EXACTE identique √† WorkerLocal
```

**Impact** :
- Versions diff√©rentes = commits `llama.cpp` sous-jacents diff√©rents
- Flags de compilation diff√©rents (Windows vs Linux)
- **Embeddings g√©n√©r√©s diff√©rents** malgr√© m√™mes param√®tres
- Distances incompatibles ‚Üí 0 r√©sultats

**Solution finale (commit `66d7f5d` - 13 oct 2025)** :

```python
# Agent-Orchestrator/backend/requirements.txt
--no-binary=llama-cpp-python
llama-cpp-python==0.3.16  # ‚úÖ VERSION EXACTE

# Agent-Orchestrator/backend/llama_server.py (ligne 41-51)
llm = llama_cpp.Llama(
    model_path=str(model_path),
    embedding=True,
    n_ctx=512,
    n_batch=2048,
    n_threads=cpu_count,
    n_ubatch=2048,
    low_vram=False,
    f16_kv=True,
    verbose=False
    # ‚úÖ STRICTEMENT IDENTIQUE √† WorkerLocal (0 param√®tre diff√©rent)
)

# G√©n√©ration embedding (ligne 60-79)
embedding_list = embedding_response['data'][0]['embedding']
return {
    "success": True,
    "embedding": embedding_list  # ‚úÖ Embedding BRUT (pas de normalisation)
}
```

**Tests de validation** :

```bash
# Test 1: Query naturelle
Query: "Quelles sont les regles urbanisme"
R√©sultats: 3 documents ‚úÖ
Distance: 0.6774 (< 0.70) ‚úÖ

# Test 2: Mot-cl√© simple
Query: "urbanisme"
R√©sultats: 5 documents ‚úÖ
Distance: 0.5977 (< 0.70) ‚úÖ

# Test 3: Auto-similarit√© PostgreSQL (pgvector)
SELECT embedding <=> embedding AS distance_self
FROM documents LIMIT 1;
R√©sultat: distance_self = 0 ‚úÖ
```

**Thresholds finaux** :
```python
# Endpoints RAG (Agent-Orchestrator/backend/api/v3/rag.py)
/search-legifrance:  similarity_threshold=0.70  # Recommand√© Solon-base
/documents/top:      similarity_threshold=0.70  # Align√©
```

**Edge Function system-tests (v8)** :
```typescript
// Query naturelle pour test r√©aliste
const response = await fetch(
  `${BACKEND}/api/v3/rag/search-legifrance?query=Quelles+sont+les+regles+urbanisme&limit=3`
);
```

**Distances observ√©es** :
- Queries naturelles : ~0.60-0.70 (plage optimale)
- Mot-cl√© simple : ~0.55-0.65 (tr√®s bon)
- Auto-similarit√© DB : 0 (parfait)

**Conclusion** :
- ‚úÖ Version llama-cpp-python **DOIT √™tre exacte** (pas `>=`)
- ‚úÖ Param√®tres Llama **strictement identiques** Backend = Workers
- ‚úÖ Pas de normalisation (embeddings bruts)
- ‚úÖ Pas de pr√©fixe "query :"
- ‚úÖ `json.dumps()` pour pgvector
- ‚úÖ Lock asyncio pour subprocess
- ‚úÖ Threshold 0.70 optimal pour Solon-base

---

### **Int√©gration RAG dans le Chatbot (13 oct 2025 - CONNEXION COMPL√àTE)** üí¨

**Objectif** : Le chatbot utilise automatiquement la recherche s√©mantique pour enrichir ses r√©ponses.

**Impl√©mentation dans l'Orchestrator** :

**Fichier** : `Agent-Orchestrator/backend/agents/orchestrator.py` (ligne 286-360)

```python
async def process_message_stream(
    self,
    user_id: str,
    session_id: str,
    messages: list,
    project_id: str | None = None,
    model: str | None = None,
) -> AsyncGenerator[str, None]:
    """
    Traite un message utilisateur en streaming avec recherche s√©mantique directe.
    """
    # ... (initialisation LLM, MCP) ...
    
    # üîç RECHERCHE S√âMANTIQUE AUTOMATIQUE
    semantic_context = ""
    try:
        # R√©cup√©rer la derni√®re question utilisateur
        user_question = ""
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                user_question = msg.content
                break
        
        if user_question:
            # ‚úÖ Recherche multi-sources avec threshold 0.70
            search_request = SearchRequest(
                query=user_question,
                mode=SearchMode.SEMANTIC,
                limit=8,  # 8 documents recherch√©s
                threshold=0.70,  # ‚úÖ FIX: Threshold optimal
                filters=None  # ‚úÖ FIX: Multi-sources (L√©gifrance + PLU + futurs)
            )
            
            search_response = await search_service.search(search_request)
            
            if search_response.results:
                context_docs = []
                legal_references = []
                
                # Top 5 r√©sultats utilis√©s
                for i, result in enumerate(search_response.results[:5]):
                    content = result.content[:400]  # 400 chars max
                    
                    # Extraction r√©f√©rences l√©gales automatique
                    # (Articles, Codes, D√©crets, Arr√™t√©s)
                    legal_patterns = [
                        r'art\.?\s*L?\d+[-\w]*',
                        r'Code\s+[A-Z][a-z]+',
                        r'D√©cret\s+n¬∞?\s*\d+',
                        r'Arr√™t√©\s+n¬∞?\s*\d+'
                    ]
                    
                    found_refs = []
                    for pattern in legal_patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        found_refs.extend(matches)
                    
                    context_docs.append(f"{i+1}. {content}...")
                    legal_references.extend(found_refs)
                
                # Construction du contexte documentaire
                semantic_context = f"""
                
CONTEXTE DOCUMENTAIRE (Recherche s√©mantique sur "{user_question[:100]}..."):

{chr(10).join(context_docs)}

R√©f√©rences l√©gales identifi√©es : {', '.join(set(legal_references[:10]))}
                """
                
                logger.info(f"üìö Contexte documentaire construit: {len(context_docs)} docs, {len(set(legal_references))} r√©f√©rences")
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Recherche s√©mantique √©chou√©e: {e}")
        # Continuer sans contexte documentaire
    
    # Injection du contexte dans le system prompt
    system_prompt = f"""Tu es ArchiReg, assistant expert en urbanisme fran√ßais.
    
{semantic_context}

EXPERTISE M√âTIER :
- Donn√©es provenant du micro-service L√©gifrance PISTE (16 codes juridiques)
- R√©f√©rences juridiques pr√©cises
...
"""
    
    # Envoyer au LLM Groq avec contexte enrichi
    # ... (streaming vers frontend)
```

**Flow complet** :

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant F as Frontend
    participant O as Orchestrator
    participant R as RAG Service
    participant DB as pgvector
    participant LLM as Groq LLM
    
    U->>F: "Quelles sont les r√®gles urbanisme ?"
    F->>O: /api/v3/chat/completions
    O->>R: Recherche s√©mantique automatique
    R->>DB: Embedding query + recherche vectorielle
    DB-->>R: 8 documents pertinents (distance < 0.70)
    R-->>O: Top 5 documents + r√©f√©rences l√©gales
    O->>O: Injection contexte dans system prompt
    O->>LLM: Messages + contexte documentaire
    LLM-->>O: R√©ponse enrichie (streaming)
    O-->>F: Streaming chunks
    F-->>U: R√©ponse avec r√©f√©rences L√©gifrance ‚úÖ
```

**Avantages** :
- ‚úÖ Recherche automatique (pas besoin d'activer manuellement)
- ‚úÖ Multi-sources (L√©gifrance + PLU + futurs)
- ‚úÖ Top 5 docs les plus pertinents
- ‚úÖ Extraction automatique r√©f√©rences l√©gales
- ‚úÖ Contexte inject√© dans chaque r√©ponse
- ‚úÖ R√©ponses enrichies avec articles de loi pr√©cis

**Logs Render attendus** :
```
üîç D√âBUT RECHERCHE S√âMANTIQUE - Question: ...
üîç Initialisation du service de recherche...
üîç Ex√©cution de la recherche s√©mantique...
üîç R√©sultats de recherche: 8 documents
üìö Contexte documentaire construit: 5 docs, X r√©f√©rences
```

**Tests** :
- ‚úÖ Test syst√®me RAG : 3 documents trouv√©s
- ‚úÖ Chatbot frontend : R√©ponses avec contexte L√©gifrance

---

### **Lazy Loading & Performance (13 oct 2025)** ‚ö°

**Principe** : Le mod√®le GGUF (500 MB) se charge **√† la premi√®re utilisation** pour √©conomiser la RAM au d√©marrage.

**Comportement** :

| √âv√©nement | Action | Temps |
|-----------|--------|-------|
| **D√©marrage Backend** | Initialisation services SANS charger le mod√®le | ~5s |
| **Premi√®re requ√™te RAG** | T√©l√©chargement + Chargement mod√®le GGUF | ~13-15s ‚è≥ |
| **Requ√™tes suivantes** | Mod√®le d√©j√† charg√© en m√©moire | ~1-2s ‚úÖ |

**Impl√©mentation** :

```python
# services/supabase_embedding_service.py (ligne 56-70)
async def initialize(self):
    """Initialisation lazy du service"""
    if self._initialized:
        return
    
    logger.info("üîÑ Initialisation lazy GGUFEmbeddingService...")
    
    # 1. T√©l√©charger mod√®le si n√©cessaire (~2s)
    await self._download_model_if_needed()
    
    # 2. Lancer subprocess llama-cpp (~13s premi√®re fois)
    await self._start_llama_subprocess()
    
    self._initialized = True
    logger.info("‚úÖ GGUFEmbeddingService pr√™t!")
```

**Avantages** :
- ‚úÖ D√©marrage backend rapide (pas de timeout Render)
- ‚úÖ RAM √©conomis√©e si service pas utilis√©
- ‚úÖ Model partag√© entre toutes les requ√™tes

**Inconv√©nient** :
- ‚è≥ Premi√®re requ√™te lente (~15s)
- Solution : Warm-up endpoint (optionnel futur)

**Logs** :
```
[info] üîÑ Initialisation lazy du service embedding...
[info] üì• T√©l√©chargement mod√®le GGUF...
[info] üîÑ Chargement mod√®le llama-cpp via subprocess...
[info] ‚úÖ Mod√®le llama-cpp charg√© avec succ√®s
[info] ‚úÖ Subprocess confirm√© ready!
```

---

### **Lock Asyncio Subprocess (13 oct 2025 - FIX RACE CONDITION)** üîí

**Probl√®me** : Si 2+ requ√™tes arrivent simultan√©ment, elles tentent de lire le subprocess en parall√®le ‚Üí **CRASH** !

**Erreur observ√©e** :
```
readuntil() called while another coroutine is already waiting for incoming data
```

**Solution** : Lock asyncio pour synchroniser l'acc√®s au subprocess.

**Impl√©mentation** :

```python
# services/supabase_embedding_service.py (ligne 49-54)
class GGUFEmbeddingService:
    def __init__(self):
        # ... autres attributs ...
        self._subprocess_lock = asyncio.Lock()  # ‚úÖ FIX: Lock pour √©viter race condition

# services/supabase_embedding_service.py (ligne 244-274)
async def _call_llama_subprocess(self, text: str) -> list[float]:
    """Appelle le subprocess llama-cpp pour g√©n√©rer un embedding"""
    
    # ‚úÖ FIX: Lock garantit 1 seule requ√™te √† la fois
    async with self._subprocess_lock:
        try:
            # Pr√©parer requ√™te JSON
            request = {"text": text}
            request_json = json.dumps(request) + "\n"
            
            # Envoyer au subprocess
            self._llama_process.stdin.write(request_json.encode())
            await self._llama_process.stdin.drain()
            
            # Lire r√©ponse (atomique, pas de race condition)
            response_line = await self._llama_process.stdout.readline()
            response = json.loads(response_line.decode().strip())
            
            return response["embedding"]
        
        except Exception as e:
            logger.error(f"‚ùå Erreur communication subprocess: {e}")
            raise
```

**Pourquoi n√©cessaire** :
- Subprocess = 1 seul processus partag√©
- stdin/stdout = 1 seule pipe
- Si 2 requ√™tes lisent en parall√®le ‚Üí m√©lange des r√©ponses ‚ùå
- Lock garantit ordre s√©quentiel ‚úÖ

**Impact performance** :
- Minimal (~0-50ms latency ajout√©e)
- √âvite crashes critiques
- Garantit fiabilit√© 100%

---

### **Table `parsed_files` (Anti-duplication)** ‚úÖ

```sql
CREATE TABLE parsed_files (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_path TEXT NOT NULL UNIQUE,
  content_hash TEXT NOT NULL,
  worker_id TEXT NOT NULL,
  status TEXT DEFAULT 'completed',  -- 'completed', 'failed'
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  CONSTRAINT unique_file_path UNIQUE (file_path)
);

-- Index pour v√©rification rapide des doublons
CREATE INDEX idx_parsed_files_path ON parsed_files(file_path);
CREATE INDEX idx_parsed_files_hash ON parsed_files(content_hash);
```

**R√¥le** : √âviter de reparser les m√™mes fichiers

**Statistiques (12 oct 2025)** :
- **Rows** : 930,937 fichiers pars√©s
- **Usage** : Anti-duplication 99.9% efficace

---

## üß† Mod√®le GGUF : SOLON-EMBEDDINGS-BASE

### **Caract√©ristiques**

| Propri√©t√© | Valeur |
|-----------|--------|
| **Nom** | Solon-embeddings-base-0.1.Q8_0.gguf |
| **Dimensions** | 768 |
| **Taille** | ~500 MB |
| **Quantization** | Q8_0 (8-bit) |
| **Langue** | Fran√ßais (optimis√©) |
| **Sp√©cialisation** | Textes juridiques fran√ßais |
| **Format** | GGUF (llama.cpp compatible) |

### **Stockage**

**Bucket Supabase** :
- **Nom** : `ai-models`
- **Fichier** : `Solon-embeddings-base-0.1.Q8_0.gguf`
- **URL publique** : `https://joozqsjbcwrqyeqepnev.supabase.co/storage/v1/object/public/ai-models/Solon-embeddings-base-0.1.Q8_0.gguf`

**Cache Local** :
- **Backend** : `Agent-Orchestrator/backend/cache/shared/`
- **Worker** : `WorkerLocal/models/`

### **Chargement Backend (Lazy Loading)**

```python
# services/supabase_embedding_service.py
class GGUFEmbeddingService:
    async def _download_model_if_needed(self):
        """T√©l√©charge le mod√®le si absent localement"""
        if not self.local_cache_path.exists():
            logger.info("üì• T√©l√©chargement mod√®le GGUF...")
            async with aiohttp.ClientSession() as session:
                async with session.get(self.supabase_storage_url) as response:
                    if response.status == 200:
                        with open(self.local_cache_path, "wb") as f:
                            async for chunk in response.content.iter_chunked(8192):
                                f.write(chunk)
                        logger.info("‚úÖ Mod√®le t√©l√©charg√©")
```

### **Chargement Worker (Startup)**

```python
# embedding/llama_service.py
async def _load_model(self):
    """Charge le mod√®le llama-cpp"""
    self.llm = Llama(
        model_path=str(self.model_path),
        embedding=True,
        n_ctx=512,               # ‚úÖ FIX: Align√© avec n_ctx_train (qualit√© 100%)
        n_batch=2048,            # Batch MASSIF
        n_threads=cpu_count,     # Tous les cores
        n_ubatch=2048,           # Batch unitaire
        low_vram=False,          # Utiliser toute la RAM
        f16_kv=True,             # Float16 pour perfs
        verbose=False
    )
```

### **Chargement Backend (Subprocess llama_server.py)**

```python
# llama_server.py
def load_llama_model():
    """Charge le mod√®le llama-cpp avec param√®tres optimis√©s pour Render"""
    llm = llama_cpp.Llama(
        model_path=str(model_path),
        embedding=True,  # MODE EMBEDDING OBLIGATOIRE
        n_ctx=512,       # ‚úÖ FIX: ALIGN√â avec n_ctx_train (qualit√© 100%)
        n_batch=2048,    # ‚úÖ Batch MASSIF pour performance
        n_threads=cpu_count,  # ‚úÖ TOUS les cores
        n_ubatch=2048,   # ‚úÖ Micro-batch MASSIF
        verbose=False,   # Pas de logs verbeux
        # OPTIMISATIONS RENDER 2GB RAM :
        n_gpu_layers=0,  # Force CPU (pas GPU sur Render)
        use_mmap=True,   # Memory mapping efficace
        use_mlock=False, # Pas de memory lock (probl√©matique conteneurs)
        f16_kv=True,     # ‚úÖ Float16 pour performance
        logits_all=False, # Pas besoin logits pour embeddings
        vocab_only=False, # Chargement mod√®le complet n√©cessaire
        seed=-1,         # Seed al√©atoire
        low_vram=False,  # ‚úÖ D√©sactiver mode restrictif
        numa=False,      # D√©sactiver NUMA
    )
    return llm
```

**‚ö†Ô∏è IMPORTANT FIX n_ctx** :
- ‚úÖ **Worker** : `n_ctx=512` (align√© avec n_ctx_train)
- ‚úÖ **Backend** : `n_ctx=512` (align√© avec n_ctx_train)
- ‚úÖ **Mod√®le** : `n_ctx_train=512` (bert.context_length dans metadata GGUF)
- ‚úÖ **Qualit√©** : 100% (pas de warning, pas de truncation)

**Probl√®me r√©solu** :
- ‚ùå **Avant** : `n_ctx=4096` ‚Üí Warning "exceeds trained context length" ‚Üí Qualit√© d√©grad√©e
- ‚úÖ **Apr√®s** : `n_ctx=512` ‚Üí Pas de warning ‚Üí Qualit√© 100%

**Performance Backend** :
- ‚ö° **Premi√®re requ√™te** : ~500ms (warm-up subprocess)
- ‚ö° **Requ√™tes suivantes** : ~15-20ms (subprocess chaud)
- ‚ö° **Warm-up automatique** : `get_embedding("test warm-up")` au d√©marrage

---

## üó∫Ô∏è Diagramme Flux Complet (Tables Supabase)

```mermaid
graph TB
    subgraph "COLLECTE"
        MS[Microservice L√©gifrance] -->|1. Upload JSON| BKT[Bucket: agentbasic-legifrance-raw]
        MS -->|2. INSERT status=pending| FQ[Table: files_queue]
    end
    
    subgraph "TRAITEMENT NIVEAU 1: Contexte Global"
        W1[Worker Local 1] -->|3a. SELECT pending| FQ
        W2[Worker Local 2] -->|3b. SELECT pending| FQ
        W3[Worker Local 3] -->|3c. SELECT pending| FQ
        W1 -->|4a. Download JSON| BKT
        W2 -->|4b. Download JSON| BKT
        W3 -->|4c. Download JSON| BKT
        W1 -->|5a. INSERT hash| PF[Table: parsed_files]
        W2 -->|5b. INSERT hash| PF
        W3 -->|5c. INSERT hash| PF
        W1 -->|6a. INSERT + embedding| DOCS[Table: documents<br/>embedding VECTOR 768<br/>extra_data source=workerlocal]
        W2 -->|6b. INSERT + embedding| DOCS
        W3 -->|6c. INSERT + embedding| DOCS
        W1 -->|7a. UPDATE completed| FQ
        W2 -->|7b. UPDATE completed| FQ
        W3 -->|7c. UPDATE completed| FQ
    end
    
    subgraph "TRAITEMENT NIVEAU 2: Chunks Granulaires FUTUR"
        WC1[WorkerLocal Chunk 1] -.->|8. SELECT pending chunks| FQ
        WC1 -.->|9. Download JSON| BKT
        WC1 -.->|10. SELECT parent doc| DOCS
        WC1 -.->|11. INSERT chunks + embeddings| CHK[Table: document_chunks<br/>FUTUR]
    end
    
    subgraph "RECHERCHE S√âMANTIQUE"
        FE[Frontend Chat] -->|12. Query user| BE[Backend Agent-Orchestrator]
        BE -->|13. G√©n√®re embedding query| GGUF[GGUF Service<br/>READ ONLY]
        BE -->|14. SELECT vectoriel<br/>embedding vector_cosine_ops<br/>SANS filtre source| DOCS
        BE -.->|15. SELECT vectoriel chunks FUTUR| CHK
        BE -->|16. Retour r√©sultats JSON| FE
    end
    
    style MS fill:#f59e0b,stroke:#000,stroke-width:2px
    style W1 fill:#a78bfa,stroke:#000
    style W2 fill:#a78bfa,stroke:#000
    style W3 fill:#a78bfa,stroke:#000
    style WC1 fill:#f472b6,stroke:#000,stroke-dasharray: 5 5
    style BE fill:#60a5fa,stroke:#000,stroke-width:2px
    style DOCS fill:#ef4444,stroke:#000,stroke-width:3px
    style CHK fill:#ef4444,stroke:#000,stroke-dasharray: 5 5
    style BKT fill:#10b981,stroke:#000
    style FQ fill:#facc15,stroke:#000
    style PF fill:#facc15,stroke:#000
```

### **R√¥les des tables Supabase**

| Table | R√¥le | √âcrite par | Lue par | Rows (12 oct) |
|-------|------|------------|---------|---------------|
| **files_queue** | Queue traitement + status | Microservice | Workers | 1,034,780 |
| **parsed_files** | Anti-duplication (hash) | Workers | Workers (v√©rif doublon) | 930,937 |
| **documents** | Documents + embeddings globaux | Workers | Backend (RAG) + Workers Chunk | **930,394** |
| **document_chunks** | Chunks + embeddings pr√©cis | Workers Chunk (futur) | Backend (RAG futur) | 0 (pr√™t) |

### **Colonnes critiques `documents`**

| Colonne | Type | R√¥le | Remplie par | Exemple |
|---------|------|------|-------------|---------|
| `id` | UUID | Cl√© primaire | Supabase auto | `uuid-123...` |
| `title` | TEXT | Titre document | Workers | "Code de l'urbanisme" |
| `content` | TEXT | Texte complet | Workers | Contenu JSON pars√© |
| `embedding` | VECTOR(768) | Vecteur s√©mantique | Workers (GGUF) | `[0.123, -0.456, ...]` |
| `file_path` | TEXT UNIQUE | Chemin source | Workers | `legifrance/.../xxx.json` |
| `metadata` | JSONB | M√©tadonn√©es (vide) | - | `{}` (pas utilis√©) |
| `extra_data` | JSONB | Tra√ßabilit√© source | Workers | `{"source": "workerlocal", ...}` |

**Index pgvector actif :**
```sql
CREATE INDEX idx_documents_embedding 
ON documents 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);
-- ‚úÖ Recherche vectorielle en ~15-20ms sur 930k docs
```

---

## üîÑ Flux de Donn√©es Complet

### **Phase 1 : Collecte** üì°

```
Micro-service L√©gifrance PISTE
    ‚Üì [API PISTE - Codes, D√©crets, Textes]
Fichiers JSON bruts (titre, contenu, articles, sections)
    ‚Üì [Upload HTTP]
Bucket Supabase Storage (agentbasic-legifrance-raw)
    ‚îî‚îÄ legifrance/architecture_urbanisme/
       ‚îú‚îÄ code_urbanisme_123.json
       ‚îú‚îÄ decret_456.json
       ‚îî‚îÄ ... (1,077,264 fichiers)
```

**M√©triques** :
- **Fichiers collect√©s** : 1,077,264
- **Taille totale** : 4.6 GB
- **Format** : JSON structur√©

---

### **Phase 2 : Traitement** üè≠

```
Bucket Supabase Storage
    ‚Üì [API Storage - T√©l√©chargement]
Worker Local (r√©cup√®re batch de 100)
    ‚Üì [Parse JSON - Extraction texte]
Texte complet (titre + contenu + articles)
    ‚Üì [llama-cpp-python - G√©n√©ration embedding]
Embedding GGUF (768 dimensions)
    ‚Üì [INSERT SQL]
Table documents (avec colonne embedding vector(768))
    ‚Üì [Indexation automatique]
pgvector (index ivfflat - 100 listes)
```

**M√©triques (12 oct 2025)** :
- **Documents pars√©s** : 930,394 (L√©gifrance)
- **Embeddings g√©n√©r√©s** : 930,394 vecteurs (768 dims)
- **Performance** : 50 fichiers/sec
- **Anti-duplication** : 99.9%
- **Sources** : L√©gifrance (actuel), PLU (futur)

---

### **Phase 3 : Recherche** üîç

```
Frontend (query utilisateur: "r√®gles urbanisme")
    ‚Üì [HTTP GET]
Backend /api/v3/rag/search-legifrance
    ‚Üì [GGUFEmbeddingService]
G√©n√©ration embedding query (768 dims)
    ‚Üì [SupabaseSearchService]
Recherche vectorielle pgvector
    ‚Üì [SQL - Distance cosinus]
SELECT ... ORDER BY embedding <-> query_vector
    ‚Üì [Top K r√©sultats - distance < 0.3]
10 documents les plus pertinents
    ‚Üì [HTTP Response JSON]
Frontend (affichage r√©sultats)
```

**M√©triques** :
- **Latence embedding query** : ~15ms
- **Latence recherche pgvector** : ~5ms
- **Latence totale** : ~20-30ms
- **Cache hits** : 60-70%

---

## üìä M√©triques & Performance

### **Workers Locaux**

| M√©trique | Valeur | Notes |
|----------|--------|-------|
| **Vitesse traitement** | 50 fichiers/sec | Concurrence 50 threads |
| **Embedding time** | ~20ms/doc | Mod√®le Q8_0 optimis√© |
| **Batch size** | 100 fichiers | Optimal pour pagination |
| **Timeout download** | 30s | Fichiers bucket Supabase |
| **Timeout processing** | 30s | Parsing + embedding |
| **Anti-duplication** | 99.9% | Via content_hash SHA256 |

### **Backend RAG**

| M√©trique | Valeur | Notes |
|----------|--------|-------|
| **Embedding query** | ~15-20ms | G√©n√©ration via GGUF subprocess |
| **Recherche pgvector** | ~5ms | Index ivfflat (100 listes) |
| **Latence totale** | ~20-30ms | End-to-end |
| **Cache hits** | 60-70% | Redis embedding cache |
| **Seuil distance** | <0.3 | R√©sultats pertinents |
| **Premi√®re requ√™te** | ~500ms | Warm-up subprocess llama.cpp |
| **n_ctx** | 512 | ‚úÖ Align√© avec n_ctx_train |

### **Base de Donn√©es**

| M√©trique | Valeur | Notes |
|----------|--------|-------|
| **Documents index√©s** | 930,394 | Table documents (L√©gifrance + futurs) |
| **Taille embeddings** | ~2.8 GB | 930k √ó 768 √ó 4 bytes |
| **Index pgvector** | HNSW | m=16, ef_construction=64, 1.4GB |
| **Distance metric** | Cosinus | Optimal pour embeddings |
| **Threshold** | 0.70 | Seuil de similarit√© (1-distance) |
| **ef_search** | 100 | Pr√©cision/vitesse optimale |
| **statement_timeout** | 60s | Timeout pour requ√™tes lourdes |

### **Index HNSW (D√©tails Techniques)**

| Table | Index | Taille | Rows | Usage | Statut |
|-------|-------|--------|------|-------|--------|
| **documents** | `idx_documents_embedding_hnsw` | 1.4 GB | 930,701 | Contexte global des codes juridiques | ‚úÖ ACTIF |
| **document_chunks** | `idx_document_chunks_embedding_hnsw` | 16 kB | 0 | D√©tails granulaires (articles, paragraphes) | ‚è≥ FUTUR |

**üîÆ Strat√©gie Hybride Future** :
- **Recherche large** : Index `documents` pour trouver codes pertinents (<500ms)
- **Recherche pr√©cise** : Index `document_chunks` pour article exact (<300ms)
- **Fusion intelligente** : Backend combine les 2 r√©sultats (<800ms total)
- **Auto-maintenance** : Les 2 index HNSW s'adaptent automatiquement, aucun cron job requis

---

## ‚úÖ Tests Syst√®me

### **Test RAG** üß™

**Localisation** : `DOCS-ARCHITECTURE/05-EDGE-FUNCTIONS/system-tests/index.ts`

**Fonction** :
```typescript
async function testRAG(userToken: string): Promise<TestResult> {
  // ‚úÖ Appelle le backend directement (pas d'Edge Function rag-endpoint)
  const response = await fetch(
    `${SERVICES.backend}/api/v3/rag/search-legifrance?query=test+urbanisme&limit=3`,
    {
      headers: { 
        'Authorization': `Bearer ${userToken}`,
        'Content-Type': 'application/json' 
      }
    }
  )
  
  // Validation r√©ponse
  const data = await response.json()
  const isSuccess = response.ok && data.success && data.results?.length > 0
  
  return {
    name: 'RAG Search',
    status: isSuccess ? 'success' : 'error',
    message: isSuccess 
      ? `RAG OK: ${data.results.length} documents trouv√©s` 
      : `RAG KO: ${data.error}`
  }
}
```

**Validation** :
- ‚úÖ Backend accessible
- ‚úÖ G√©n√©ration embedding query fonctionnelle
- ‚úÖ Recherche pgvector op√©rationnelle
- ‚úÖ Retour r√©sultats corrects

---

## üö® Notes Importantes

### **S√©paration des Responsabilit√©s**

| Service | R√¥le | Embeddings |
|---------|------|------------|
| **Workers** | WRITE | ‚úÖ G√©n√®re embeddings documents + INSERT |
| **Backend** | READ | ‚úÖ G√©n√®re embeddings queries + SEARCH |
| **Micro-service** | COLLECT | ‚ùå Aucun embedding |

### **Source de Donn√©es**

- ‚úÖ **Bucket L√©gifrance** = Source unique pour Workers ET Future Chunking
- ‚úÖ Pas de duplication de source
- ‚úÖ Anti-duplication via `parsed_files` (content_hash)

### **Mod√®le GGUF**

- ‚úÖ **1 seul mod√®le** : Solon-embeddings-base-0.1.Q8_0.gguf
- ‚úÖ **Partag√©** entre Workers et Backend
- ‚úÖ **Stock√©** dans bucket Supabase `ai-models`
- ‚úÖ **T√©l√©charg√©** automatiquement si absent

---

## üîÆ √âvolution Future (v2.0)

### **Chunking Granulaire & Recherche Hybride Multi-Index** üéØ

**Architecture Multi-Index HNSW** :

| Composant | Table | Index HNSW | Taille | Rows | Statut |
|-----------|-------|------------|--------|------|--------|
| **Niveau 1** | `documents` | `idx_documents_embedding_hnsw` | 1.4 GB | 930,701 | ‚úÖ ACTIF |
| **Niveau 2** | `document_chunks` | `idx_document_chunks_embedding_hnsw` | 16 kB | 0 | ‚è≥ FUTUR |

**Flux de Recherche Hybride (Future v3.0)** :

```
Question utilisateur: "R√®gles zone UA urbanisme?"
                ‚îÇ
                v
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Backend g√©n√®re embedding  ‚îÇ
    ‚îÇ (GGUF Solon 768 dims)     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                ‚îÇ
       v                v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇdocuments ‚îÇ      ‚îÇdocument_chunks‚îÇ
‚îÇ 930k docs‚îÇ      ‚îÇ ~3M chunks   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                   ‚îÇ
     v                   v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ HNSW 1.4GB  ‚îÇ   ‚îÇ HNSW ~4GB    ‚îÇ
‚îÇ <500ms      ‚îÇ   ‚îÇ <300ms       ‚îÇ
‚îÇ 5 codes     ‚îÇ   ‚îÇ 10 articles  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                  ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               v
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ FUSION INTELLIGENTE  ‚îÇ
    ‚îÇ - Contexte: 5 codes  ‚îÇ
    ‚îÇ - D√©tail: 10 articles‚îÇ
    ‚îÇ Total: <800ms        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               v
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ LLM enrichi RAG      ‚îÇ
    ‚îÇ R√©ponse ultra-pr√©cise‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Objectifs WorkerLocal Chunk (v2.0)** :
1. ‚è∏Ô∏è D√©coupage documents en chunks granulaires (articles, sections)
2. ‚è∏Ô∏è G√©n√©ration embeddings par chunk via GGUF Solon
3. ‚è∏Ô∏è Insertion dans `document_chunks` avec `document_id` parent
4. ‚è∏Ô∏è Index HNSW s'agrandit automatiquement (aucun cron job requis)

**Avantages Recherche Hybride** :
- üéØ **Pr√©cision** : Article exact au lieu de doc entier
- üéØ **Contexte** : Vue d'ensemble du code juridique
- üéØ **Performance** : 2 index HNSW en parall√®le (<800ms)
- üéØ **Auto-maintenance** : Les 2 index s'adaptent automatiquement

### **Am√©lioration Mod√®le** üöÄ

**Pistes d'am√©lioration** :
1. ‚è∏Ô∏è Test Solon-embeddings-large (1024 dims)
2. ‚è∏Ô∏è Fine-tuning sur corpus juridique ArchiReg
3. ‚è∏Ô∏è √âvaluation benchmarks (accuracy, recall@k, MRR)
4. ‚è∏Ô∏è Comparaison mod√®les (multilingual-e5, BGE-M3, etc.)

---

## üìû Support Technique

**Contacts** :
- **Projet** : ArchiReg - Assistant IA pour architectes
- **Stack** : Next.js, FastAPI, Supabase, llama-cpp-python
- **Version** : 4.0.0 FINALE

**Ressources** :
- [Documentation Supabase pgvector](https://supabase.com/docs/guides/ai/vector-embeddings)
- [Documentation llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [Solon Embeddings](https://huggingface.co/OrdalieTech/Solon-embeddings-base-0.1)

---

**üìÖ Derni√®re mise √† jour** : 11 octobre 2025 19:00 UTC  
**‚úÖ Status** : ARCHITECTURE DOCUMENT√âE + FIX n_ctx + WARM-UP VALID√âS ‚úÖ

---

## üîß CHANGELOG v2.5

### **13 octobre 2025 18:00 UTC - Optimisation HNSW + Multi-Index Future**

**Optimisations appliqu√©es** :
1. ‚úÖ `supabase_client.py` : `statement_timeout` 30s ‚Üí 60s
2. ‚úÖ `supabase_client.py` : Ajout `SET hnsw.ef_search = 100` (pr√©cision/vitesse optimale)
3. ‚úÖ `supabase_search_service.py` : Logs d√©taill√©s (temps embedding, recherche, total)
4. ‚úÖ Documentation : Architecture multi-index HNSW future (documents + document_chunks)

**Index HNSW cr√©√©s** :
- ‚úÖ `idx_documents_embedding_hnsw` : 1.4 GB, m=16, ef_construction=64, 930k docs
- ‚úÖ `idx_document_chunks_embedding_hnsw` : 16 kB, vide (pr√™t pour chunking futur)

**R√©sultats attendus** :
- üéØ Recherche s√©mantique : 30s timeout ‚Üí <1s
- üéØ Pr√©cision optimale : ef_search=100 (recommand√© Supabase)
- üéØ Strat√©gie hybride future : 2 index HNSW en parall√®le (<800ms)

**Maintenance** :
- ‚úÖ Auto-maintenance : Index HNSW s'adapte automatiquement
- ‚úÖ Aucun cron job requis : VACUUM ANALYZE hebdo suffit (jobid 3)

---

### **11 octobre 2025 - FIX n_ctx + Warm-up**

**Probl√®me identifi√©** :
- ‚ùå Backend : `n_ctx=4096` ‚Üí Warning "exceeds trained context length 512"
- ‚ùå WorkerLocal : `n_ctx=4096` ‚Üí Warning identique
- ‚ùå Premi√®re requ√™te RAG : 4.45s (subprocess lazy loading)

**Corrections appliqu√©es** :
1. ‚úÖ `backend/llama_server.py` : `n_ctx=512` (align√© avec mod√®le)
2. ‚úÖ `WorkerLocal/embedding/llama_service.py` : `n_ctx=512` (align√© avec mod√®le)
3. ‚úÖ `backend/core/startup.py` : Warm-up automatique `get_embedding("test warm-up")`
4. ‚úÖ `backend/Dockerfile` : D√©sactivation AVX2/FMA pour compatibilit√© AMD EPYC Render

**R√©sultats** :
- ‚úÖ **Qualit√© embeddings** : 100% (pas de warning, pas de truncation)
- ‚úÖ **Latence 1√®re requ√™te** : 4.45s ‚Üí ~500ms (-89%)
- ‚úÖ **Backend SIGILL** : R√©solu (compilation CPU g√©n√©rique SSE4)
- ‚úÖ **Worker mode** : `adaptive` ‚Üí `read_only`

**Validation** :
```sql
SELECT COUNT(*) as docs_over_512_tokens 
FROM documents 
WHERE LENGTH(content) > 2048;
-- R√©sultat: 0 (tous les docs < 512 tokens, pas besoin de reparser)
```

---

## üìä Index HNSW : Explication Compl√®te (13 oct 2025)

### üéØ √Ä Quoi Sert l'Index HNSW ?

**HNSW = Hierarchical Navigable Small World**

C'est un **catalogue intelligent** qui permet de trouver rapidement des documents similaires dans une base massive.

#### üìö Analogie Simple : La Biblioth√®que

**TA BASE DE DONN√âES = UNE BIBLIOTH√àQUE DE 930,000 LIVRES**

**üî¥ SANS INDEX (recherche lin√©aire) :**

```
Utilisateur : "Trouve-moi des livres sur l'urbanisme"

Biblioth√©caire (sans catalogue) :
  üìñ Livre 1 : "Cuisine fran√ßaise" ‚Üí Compare ‚Üí Distance = 0.95 ‚ùå
  üìñ Livre 2 : "Histoire de Paris" ‚Üí Compare ‚Üí Distance = 0.82 ‚ùå
  üìñ Livre 3 : "Code civil" ‚Üí Compare ‚Üí Distance = 0.74 ‚ùå
  ... (930,000 comparaisons)
  üìñ Livre 500,000 : "Urbanisme Lyon" ‚Üí Compare ‚Üí Distance = 0.45 ‚úÖ
  ... (430,000 comparaisons restantes)
  üìñ Livre 930,000 : "Cuisine italienne" ‚Üí Compare ‚Üí Distance = 0.98 ‚ùå

‚è±Ô∏è Temps : 30+ secondes ‚Üí TIMEOUT ‚ùå
üìä Calculs : 930,000 comparaisons (une par document)
üéØ R√©sultat : Impossible √† utiliser en production
```

**üü¢ AVEC INDEX HNSW (recherche intelligente) :**

```
Utilisateur : "Trouve-moi des livres sur l'urbanisme"

Biblioth√©caire (avec catalogue HNSW) :
  üìä Consulte le CATALOGUE INTELLIGENT
     ‚Üí Niveau 3 : Identifie section "Droit & Urbanisme" (10 sauts)
     ‚Üí Niveau 2 : Affine vers "Code urbanisme" (30 sauts)  
     ‚Üí Niveau 1 : Recherche fine dans candidats (100 comparaisons)
  
  üìñ Livre 156,234 : "Urbanisme Lyon" ‚Üí Distance = 0.45 ‚úÖ
  üìñ Livre 234,567 : "Code urbanisme" ‚Üí Distance = 0.38 ‚úÖ
  üìñ Livre 789,123 : "PLU Paris" ‚Üí Distance = 0.52 ‚úÖ

‚è±Ô∏è Temps : <1 seconde ‚úÖ
üìä Calculs : ~150 comparaisons (au lieu de 930,000)
üéØ Gain : 6,200x moins de calculs !
```

### üî¨ Comment √áa Marche Techniquement ?

**HNSW construit un GRAPHE MULTI-NIVEAUX :**

```mermaid
graph TB
    subgraph "NIVEAU 3 - Entr√©e Rapide (10 n≈ìuds)"
        A[Doc A<br/>Urbanisme] <==> B[Doc B<br/>Construction]
        B <==> C[Doc C<br/>PLU]
    end
    
    subgraph "NIVEAU 2 - Navigation (100 n≈ìuds)"
        A --> D[Doc D<br/>Permis]
        A --> E[Doc E<br/>Zonage]
        B --> F[Doc F<br/>Code civil]
        C --> G[Doc G<br/>R√®glement]
    end
    
    subgraph "NIVEAU 1 - Recherche Fine (930k n≈ìuds)"
        D --> H[Doc H]
        D --> I[Doc I]
        E --> J[Doc J]
        E --> K[Doc K]
        F --> L[Doc L]
        G --> M[Doc M]
        G --> N[Doc N]
    end
    
    style A fill:#4CAF50
    style D fill:#8BC34A
    style H fill:#CDDC39
```

**PHASES DE RECHERCHE :**

1. **Niveau 3 (Entr√©e rapide)** : ~10 sauts pour trouver la zone g√©n√©rale
2. **Niveau 2 (Navigation)** : ~30 sauts pour affiner
3. **Niveau 1 (Recherche fine)** : ~100-150 comparaisons finales

**TOTAL : ~150 op√©rations au lieu de 930,000 !**

### üìê Param√®tres Optimis√©s

**Configuration utilis√©e** :

```sql
CREATE INDEX idx_documents_embedding_hnsw 
ON documents 
USING hnsw (embedding vector_cosine_ops)
WITH (
    m = 16,              -- Connexions par n≈ìud
    ef_construction = 64 -- Qualit√© construction
);
```

**Explication** :

- **`m = 16`** : Chaque n≈ìud est connect√© √† 16 autres n≈ìuds
  - Plus √©lev√© = meilleure pr√©cision, mais index plus gros
  - 16 = bon √©quilibre pour 930k docs
  
- **`ef_construction = 64`** : Taille de la file lors de la construction
  - Plus √©lev√© = meilleure qualit√© d'index, mais construction plus lente
  - 64 = qualit√© optimale sans trop ralentir

- **`vector_cosine_ops`** : Utilise la distance cosinus (identique √† `<=>`)

### üìä Gains de Performance Attendus

| M√©trique | Sans Index | Avec HNSW | Gain |
|----------|-----------|-----------|------|
| **Comparaisons** | 930,000 | ~150 | **6,200x** |
| **Temps recherche** | 30s+ (timeout) | <1s | **100x-1000x** |
| **Complexit√© algo** | O(n) lin√©aire | O(log n) logarithmique | **Exponentiel** |
| **Coverage** | 21% (200k docs) | 100% (930k docs) | **479%** |
| **Utilisabilit√©** | ‚ùå Timeout | ‚úÖ Production | **Op√©rationnel** |

### ‚è≥ Construction de l'Index (13 oct 2025)

**√âtat actuel** (v√©rification en temps r√©el) :

```json
{
  "index_name": "idx_documents_embedding_hnsw",
  "index_size": "199 MB",
  "is_valid": false,     // ‚è≥ PAS ENCORE PR√äT
  "is_ready": false,     // ‚è≥ EN CONSTRUCTION
  "estimated_rows": 930701
}
```

**PHASES DE CONSTRUCTION `CREATE INDEX CONCURRENTLY` :**

```
Phase 1: Scan initial (lecture table)        [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 40%
Phase 2: Build HNSW graph (construction)     [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 60% ‚Üê ACTUEL
Phase 3: Finalisation (optimisation)         [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 80%
Phase 4: Validation (activation index)       [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100%
```

**Dur√©e par phase (930k docs √ó 768 dims)** :
- Phase 1 : ~3-5 minutes (scan s√©quentiel)
- Phase 2 : ~5-10 minutes (construction graphe HNSW) ‚Üê **EN COURS**
- Phase 3 : ~2-3 minutes (optimisation connexions)
- Phase 4 : ~30 secondes (validation finale)

**TOTAL ESTIM√â : 10-20 minutes**

**Surveillance de la progression** :

```sql
-- V√©rifier l'√©tat toutes les 2-3 minutes
SELECT 
    i.relname,
    pg_size_pretty(pg_relation_size(i.oid)) as size,
    idx.indisvalid as valid,
    idx.indisready as ready
FROM pg_class i
JOIN pg_index idx ON i.oid = idx.indexrelid
WHERE i.relname = 'idx_documents_embedding_hnsw';
```

**QUAND `is_valid = true` ET `is_ready = true` :**
- ‚úÖ Index ACTIF et utilisable
- ‚úÖ PostgreSQL l'utilisera automatiquement
- ‚úÖ Recherche <1s sur 930k docs

### üîß Pourquoi l'Index N'est Pas Encore Utilis√© ?

**PostgreSQL refuse d'utiliser un index `CONCURRENTLY` tant qu'il n'est pas compl√®tement valid√©.**

**Raison** : Garantir la coh√©rence des donn√©es
- Index invalide = r√©sultats potentiellement incorrects
- PostgreSQL pr√©f√®re faire une recherche lin√©aire (lente mais correcte)
- Une fois valid√© ‚Üí utilisation automatique

**CE QUI SE PASSE MAINTENANT :**

```
10:42:09 - üöÄ Recherche vectorielle HNSW activ√©e (930k docs, <1s) ‚úÖ
           ‚Üì
10:42:09 - PostgreSQL v√©rifie l'index
           ‚Üì
           Index is_valid = false ‚ùå
           ‚Üì
           PostgreSQL IGNORE l'index
           ‚Üì
           Recherche lin√©aire sur 930k docs
           ‚Üì
10:42:39 - TIMEOUT (30s d√©pass√©s) ‚ùå
```

**APR√àS VALIDATION (is_valid = true) :**

```
Future - üöÄ Recherche vectorielle HNSW activ√©e (930k docs, <1s) ‚úÖ
           ‚Üì
Future - PostgreSQL v√©rifie l'index
           ‚Üì
           Index is_valid = true ‚úÖ
           ‚Üì
           PostgreSQL UTILISE l'index HNSW
           ‚Üì
           Recherche HNSW sur ~150 vecteurs
           ‚Üì
Future - R√©sultats en <1s ‚úÖ
```

---

## ‚ö° Optimisation Performance (13 oct 2025 - CRITIQUE)

### üêõ Probl√®me Identifi√©

**Sympt√¥me** : Recherche vectorielle timeout (>30s) ‚Üí Chatbot ne peut pas utiliser RAG

**Cause** : 930,394 documents sans index HNSW = **recherche lin√©aire compl√®te**
- Chaque recherche calcule la distance cosinus pour TOUS les documents
- Temps: O(n) o√π n = 930k ‚Üí timeout Supabase (30s)

### ‚úÖ Fix Temporaire (EN PRODUCTION)

**Fichier** : `Agent-Orchestrator/backend/services/supabase_search_service.py`  
**Lignes** : 261-272  
**Date** : 13 oct 2025 12:11 UTC  
**Commit** : `b704cc1`

```python
# ‚ö° OPTIMISATION TEMPORAIRE (en attendant index HNSW)
# Limiter la recherche aux 200k documents les plus r√©cents
where_conditions.append(
    f"""id IN (
        SELECT id FROM documents 
        ORDER BY upload_date DESC NULLS LAST 
        LIMIT 200000
    )"""
)
```

**R√©sultats** :
- ‚úÖ Temps : 30s+ ‚Üí 5-10s estim√©
- ‚úÖ Coverage : 21% de la base (docs les plus r√©cents)
- ‚ö†Ô∏è Limitation : N'acc√®de pas aux 730k documents plus anciens

### üöÄ Solution Permanente : Index HNSW

**Fichier** : `DOCS-ARCHITECTURE/CREATE_HNSW_INDEXES.sql`  
**Status** : ‚ö†Ô∏è **√Ä EX√âCUTER MANUELLEMENT**

#### Ex√©cution

1. Ouvrir SQL Editor Supabase : https://supabase.com/dashboard/project/joozqsjbcwrqyeqepnev/sql/new
2. Copier-coller le contenu de `CREATE_HNSW_INDEXES.sql`
3. Cliquer RUN (dur√©e : 5-15 minutes)

#### Gains Attendus

| M√©trique | Avant (sans index) | Apr√®s (HNSW) | Gain |
|----------|-------------------|--------------|------|
| Temps recherche | 30s+ (timeout) | <1s | **100x-1000x** |
| Coverage | 21% (200k docs) | 100% (930k docs) | **479%** |
| Complexit√© | O(n) = 930k | O(log n) | **Logarithmique** |

### üîÑ Apr√®s Cr√©ation Index HNSW

1. Retirer la limitation temporaire (lignes 261-272 de `supabase_search_service.py`)
2. V√©rifier les performances : recherche < 1s
3. Commit de nettoyage

---

## üîÑ Maintenance Index HNSW (13 oct 2025)

### ‚úÖ Mise √† Jour Automatique

**Question** : L'index HNSW se met-il √† jour automatiquement quand j'ins√®re de nouveaux documents avec embeddings ?

**R√©ponse** : **OUI, AUTOMATIQUEMENT !** ‚úÖ

**Processus** :

```sql
-- Worker ins√®re nouveau document
INSERT INTO documents (title, content, embedding)
VALUES ('Nouveau Code', 'Article L152-1...', '[0.123, -0.456, ...]');
```

**PostgreSQL fait automatiquement** :

1. ‚úÖ Insert document dans table `documents`
2. ‚úÖ Calcule position optimale dans graphe HNSW
3. ‚úÖ Cr√©e connexions avec n≈ìuds proches (m=16 connexions)
4. ‚úÖ Met √† jour l'index HNSW
5. ‚úÖ Document **imm√©diatement cherchable** via recherche vectorielle

**Temps ajout√©** : +0.1-0.5s par insertion (overhead index)

**Aucune action manuelle requise !** Les nouveaux embeddings sont automatiquement index√©s.

---

### ‚ö†Ô∏è D√©gradation Progressive

**Probl√®me** : Apr√®s des milliers d'insertions, l'index HNSW peut se fragmenter.

**Sympt√¥mes** :
- Recherche ralentit progressivement : <1s ‚Üí 2-3s
- Qualit√© r√©sultats l√©g√®rement d√©grad√©e
- Graphe HNSW sous-optimal

**Cause** :
- Insertions successives cr√©ent des connexions locales sous-optimales
- Le graphe n'est plus globalement optimal
- Certains n≈ìuds sont mal connect√©s

**Seuil critique** : Apr√®s **+100,000 insertions** ou **+10% de la base**

---

### üîß Solution : REINDEX P√©riodique

**Commande** :

```sql
-- Reconstruire l'index HNSW de fa√ßon optimale
REINDEX INDEX CONCURRENTLY idx_documents_embedding_hnsw;
```

**Effet** :
- ‚úÖ Reconstruit graphe HNSW complet
- ‚úÖ Reconnecte tous les n≈ìuds de fa√ßon optimale
- ‚úÖ Restaure performance <1s
- ‚úÖ Am√©liore qualit√© r√©sultats

**Dur√©e** : 10-20 minutes (selon taille base)

**Downtime** : **AUCUN** (gr√¢ce √† `CONCURRENTLY`)
- PostgreSQL utilise l'ancien index pendant reconstruction
- Bascule automatiquement sur le nouvel index quand pr√™t

---

### üìÖ Cron Job Recommand√©

**Cr√©er un cron job mensuel** :

```sql
-- Cr√©er cron job REINDEX mensuel (1er du mois √† 3h du matin)
SELECT cron.schedule(
    'monthly-reindex-hnsw',
    '0 3 1 * *',
    $$
    -- REINDEX index HNSW documents
    REINDEX INDEX CONCURRENTLY idx_documents_embedding_hnsw;
    
    -- REINDEX index HNSW chunks (quand actif)
    REINDEX INDEX CONCURRENTLY idx_document_chunks_embedding_hnsw;
    
    -- Mettre √† jour statistiques
    ANALYZE documents;
    ANALYZE document_chunks;
    $$
);
```

**V√©rifier les cron jobs** :

```sql
SELECT * FROM cron.job WHERE jobname LIKE '%reindex%';
```

**Logs d'ex√©cution** :

```sql
SELECT * FROM cron.job_run_details 
WHERE jobid = (SELECT jobid FROM cron.job WHERE jobname = 'monthly-reindex-hnsw')
ORDER BY start_time DESC 
LIMIT 10;
```

---

### üìä Monitoring Sant√© Index

**V√©rifier si REINDEX n√©cessaire** :

```sql
-- Comparer performance actuelle vs optimale
WITH test_query AS (
    SELECT '[0.1, 0.2, ...]'::vector as query_embedding
)
EXPLAIN ANALYZE
SELECT id, embedding <=> (SELECT query_embedding FROM test_query) as distance
FROM documents
WHERE embedding <=> (SELECT query_embedding FROM test_query) < 0.3
ORDER BY distance
LIMIT 8;
```

**Indicateurs** :
- ‚úÖ Temps < 500ms : Index optimal
- ‚ö†Ô∏è Temps 500ms-2s : Index OK mais peut √™tre optimis√©
- ‚ùå Temps > 2s : **REINDEX recommand√©**

**V√©rifier fragmentation** :

```sql
-- Nombre de scans index vs table
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched,
    CASE 
        WHEN idx_scan > 0 THEN ROUND(100.0 * idx_tup_fetch / idx_tup_read, 2)
        ELSE 0
    END as efficiency_pct
FROM pg_stat_user_indexes
WHERE indexname LIKE '%hnsw%';
```

---

### üìã Checklist Maintenance Index HNSW

**Mensuel** :
- [ ] REINDEX CONCURRENTLY (cron job automatique)
- [ ] ANALYZE tables
- [ ] V√©rifier logs cron job
- [ ] Tester temps recherche (<500ms)

**Apr√®s √©v√©nement majeur** :
- [ ] REINDEX si +100k insertions d'un coup
- [ ] REINDEX si changement version pgvector
- [ ] REINDEX si migration donn√©es

**Monitoring continu** :
- [ ] Temps recherche < 1s
- [ ] Index size stable (~200 MB pour 930k docs)
- [ ] Efficiency > 95%

---

## üìö R√©f√©rences

- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [pgvector HNSW Indexes](https://github.com/pgvector/pgvector#hnsw)
- [llama.cpp Documentation](https://github.com/ggerganov/llama.cpp)
- [Hugging Face - Solon Embeddings](https://huggingface.co/OrdalieTech/Solon-embeddings-base-0.1)

