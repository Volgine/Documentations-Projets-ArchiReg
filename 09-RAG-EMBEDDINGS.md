# ğŸ§  RAG & EMBEDDINGS - Architecture ComplÃ¨te

**Date de crÃ©ation** : 11 octobre 2025  
**DerniÃ¨re mise Ã  jour** : 13 octobre 2025 18:00 UTC  
**Version** : 2.5 HNSW OPTIMISÃ‰  
**Status** : âœ… **RAG ACTIF** - Index HNSW (1.4GB, 930k docs, <1s) + Optimisations Supabase (timeout 60s, ef_search 100)

---

## ğŸ¯ Vue d'Ensemble

Ce document dÃ©crit l'architecture complÃ¨te du systÃ¨me **RAG (Retrieval-Augmented Generation)** et de **gÃ©nÃ©ration d'embeddings** pour la recherche sÃ©mantique dans les documents juridiques LÃ©gifrance.

---

## ğŸ“Š Architecture Globale

```mermaid
graph TB
    subgraph "1ï¸âƒ£ COLLECTE DONNÃ‰ES"
        A[Micro-service LÃ©gifrance PISTE] -->|Upload JSON| B[Bucket Supabase Storage<br/>agentbasic-legifrance-raw]
    end
    
    subgraph "2ï¸âƒ£ TRAITEMENT: WORKERS LOCAUX"
        B -->|TÃ©lÃ©charge fichiers| C[Worker Local x3<br/>ACTUEL v1.0]
        C -->|Parse + GGUF Embedding| D[(Table documents<br/>1 doc = 1 embedding contexte)]
        D -->|Vecteurs 768 dims| E[pgvector extension]
        
        H[Worker Local FUTUR<br/>v2.0 Chunking] -.->|Parse + Chunks| I[(Table document_chunks<br/>Embeddings granulaires)]
        B -.->|Source future| H
        I -.->|Vecteurs 768 dims| E
    end
    
    subgraph "3ï¸âƒ£ BACKEND: RECHERCHE SÃ‰MANTIQUE"
        F[Backend Agent-Orchestrator] -->|READ ONLY| E
        F -->|GÃ©nÃ¨re embedding query| K[GGUF Model Local<br/>Solon-embeddings-base-0.1.Q8_0.gguf]
        K -->|TÃ©lÃ©chargÃ© depuis| L[Bucket Supabase<br/>ai-models]
        F -->|Recherche vectorielle| E
        F -->|API RAG| G[Frontend Chat]
    end
    
    style C fill:#4ecdc4
    style F fill:#ff6b6b
    style H fill:#95e1d3,stroke-dasharray: 5 5
    style I fill:#95e1d3,stroke-dasharray: 5 5
    style K fill:#ffd93d
    style L fill:#ffd93d
```

---

## ğŸ—ï¸ Composants et ResponsabilitÃ©s

### 1ï¸âƒ£ **Micro-service LÃ©gifrance** (@Micro-service-data-legifrance-piste/)

**URL** : `https://micro-service-data-legifrance-piste.onrender.com`

**RÃ´le** : Collecteur de donnÃ©es juridiques depuis API PISTE

**Actions** :
- âœ… Appel API PISTE (codes, textes juridiques, dÃ©crets)
- âœ… TÃ©lÃ©chargement fichiers JSON bruts
- âœ… Upload dans bucket Supabase `agentbasic-legifrance-raw`
- âœ… Alimentation continue des donnÃ©es

**âš ï¸ IMPORTANT** :
- âŒ **PAS** de parsing
- âŒ **PAS** de gÃ©nÃ©ration d'embeddings
- âŒ **PAS** d'accÃ¨s Ã  la base de donnÃ©es
- âœ… **SEULEMENT** collecte et upload

**Output** : Fichiers JSON dans `agentbasic-legifrance-raw/legifrance/architecture_urbanisme/`

---

### 2ï¸âƒ£ **Worker Local** (@WorkerLocal/) ğŸ­

**Localisation** : PC Windows (3 instances parallÃ¨les)

#### **VERSION ACTUELLE (v1.0 - Embeddings Contexte Global)**

**RÃ´le** : Parser les documents et gÃ©nÃ©rer les embeddings de contexte global

**Flux de traitement** :
1. âœ… RÃ©cupÃ¨re fichiers JSON depuis bucket LÃ©gifrance (API Supabase Storage)
2. âœ… Parse le JSON :
   - Extraction titre
   - Extraction contenu principal
   - Extraction articles
   - Extraction sections
   - DÃ©doublonnage automatique
3. âœ… GÃ©nÃ¨re **1 embedding GGUF par document complet** (contexte global)
4. âœ… INSERT dans table `documents` avec colonne `embedding vector(768)`
5. âœ… Anti-duplication via table `parsed_files` (content_hash SHA256)

**ModÃ¨le utilisÃ©** :
- **Nom** : `Solon-embeddings-base-0.1.Q8_0.gguf`
- **Dimensions** : 768
- **Taille** : ~500 MB
- **Localisation** : `WorkerLocal/models/`
- **Chargement** : `llama-cpp-python` avec configuration TURBO

**Configuration Performance** :
```python
{
    "batch_size": 100,           # Fichiers traitÃ©s par batch
    "max_concurrency": 50,       # Threads parallÃ¨les
    "n_ctx": 512,                # âœ… FIX: Contexte alignÃ© avec n_ctx_train du modÃ¨le (qualitÃ© 100%)
    "n_batch": 2048,             # Batch MASSIF
    "n_threads": cpu_count,      # Tous les cores
    "n_ubatch": 2048,            # Micro-batch MASSIF
    "f16_kv": True,              # Float16 pour performance
    "low_vram": False,           # Utiliser toute la RAM disponible
    "download_timeout": 30,      # Timeout download (s)
    "processing_timeout": 30     # Timeout processing (s)
}
```

**Performance MesurÃ©e** :
- âš¡ **50 fichiers/sec** (concurrence 50)
- âš¡ **~20ms par embedding** (modÃ¨le Q8_0 optimisÃ©)
- âš¡ **99.9% anti-duplication** (content_hash + parsed_files)

**Output** : Table `documents` avec 930,394 documents indexÃ©s (LÃ©gifrance)

---

#### **VERSION FUTURE (v2.0 - Chunking Granulaire)** ğŸ”®

**RÃ´le** : DÃ©couper les documents en chunks et gÃ©nÃ©rer des embeddings granulaires

**Flux de traitement prÃ©vu** :
1. â¸ï¸ MÃªme source (bucket LÃ©gifrance)
2. â¸ï¸ Parse JSON en chunks granulaires :
   - 1 chunk par article
   - 1 chunk par section
   - 1 chunk par paragraphe (si >500 tokens)
3. â¸ï¸ GÃ©nÃ¨re **1 embedding GGUF par chunk**
4. â¸ï¸ INSERT dans table `document_chunks` avec lien `document_id`
5. â¸ï¸ Conservation du lien parent-enfant

**Avantages** :
- ğŸ¯ Recherche plus prÃ©cise (article spÃ©cifique)
- ğŸ¯ Meilleure pertinence pour queries courtes
- ğŸ¯ RÃ©duction du bruit (contexte trop large)

**Output prÃ©vu** : Table `document_chunks` (FUTUR)

---

### 3ï¸âƒ£ **Backend Agent-Orchestrator** (@Agent-Orchestrator/) ğŸ”´

**URL** : `https://agent-orchestrateur-backend.onrender.com`

**RÃ´le** : Chatbot + Recherche sÃ©mantique **READ ONLY**

**Services ClÃ©s** :
- `GGUFEmbeddingService` : GÃ©nÃ©ration embeddings queries
- `SupabaseSearchService` : Recherche vectorielle pgvector
- `RAGService` : Orchestration RAG complÃ¨te

**Flux de traitement** :
1. âœ… **TÃ©lÃ©chargement modÃ¨le** (lazy loading au dÃ©marrage) :
   - Source : `https://joozqsjbcwrqyeqepnev.supabase.co/storage/v1/object/public/ai-models/Solon-embeddings-base-0.1.Q8_0.gguf`
   - Cache local : `backend/cache/shared/`
   - Chargement subprocess `llama-cpp-python`

2. âœ… **RÃ©ception query utilisateur** :
   ```typescript
   GET /api/v3/rag/search-legifrance?query=rÃ¨gles+urbanisme&limit=10
   ```

3. âœ… **GÃ©nÃ©ration embedding query** :
   ```python
   embedding_service = get_embedding_service()
   query_embedding = await embedding_service.get_embedding("rÃ¨gles urbanisme")
   # â†’ [0.123, -0.456, 0.789, ...] (768 dimensions)
   ```

4. âœ… **Recherche vectorielle pgvector** :
   ```sql
   SELECT 
     id, 
     title, 
     content, 
     embedding <-> $1 AS distance
   FROM documents
   WHERE embedding <-> $1 < 0.3
   ORDER BY distance ASC
   LIMIT 10;
   ```

5. âœ… **Retour rÃ©sultats** :
   ```json
   {
     "success": true,
     "results": [
       {
         "id": "uuid-123",
         "title": "Code de l'urbanisme - Article L123-1",
         "content": "...",
         "distance": 0.15,
         "score": 0.85
       }
     ],
     "total_found": 10,
     "query": "rÃ¨gles urbanisme"
   }
   ```

**âš ï¸ IMPORTANT** :
- âŒ **PAS** de gÃ©nÃ©ration d'embeddings pour documents (Workers !)
- âŒ **PAS** de parsing de fichiers
- âŒ **PAS** d'Ã©criture dans `documents` ou `document_chunks`
- âœ… **SEULEMENT** gÃ©nÃ©ration embedding query + recherche vectorielle

**Endpoints API** :
- `GET /api/v3/rag/search-legifrance` : Recherche sÃ©mantique
- `GET /api/v3/rag/legifrance-stats` : Statistiques documents
- `GET /api/v3/rag/documents/top` : Top documents projet

**Performance** :
- âš¡ **~15-20ms** gÃ©nÃ©ration embedding query (aprÃ¨s warm-up)
- âš¡ **~5ms** recherche pgvector (10 rÃ©sultats)
- âš¡ **~20-30ms** latence totale end-to-end
- âš¡ **60-70%** cache hits (embeddings queries)
- ğŸ”¥ **Warm-up automatique** : RÃ©duit latence 1Ã¨re requÃªte de 4.45s Ã  500ms

---

### 4ï¸âƒ£ **Frontend** (@ArchiReg-Front/) ğŸŒ

**URL** : `https://archireg-front.vercel.app`

**RÃ´le** : Interface utilisateur chat + recherche

**Interactions RAG** :
```typescript
// Chat avec RAG
const response = await fetch('/api/v3/rag/search-legifrance', {
  method: 'GET',
  headers: {
    'Authorization': `Bearer ${userToken}`,
    'Content-Type': 'application/json'
  },
  params: {
    query: userQuery,
    limit: 10,
    similarity_threshold: 0.7
  }
})

const { results } = await response.json()
// Affichage des rÃ©sultats pertinents
```

---

## ğŸ—„ï¸ Structure Base de DonnÃ©es

### **Table `documents` (ACTUELLE)** âœ…

```sql
CREATE TABLE documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  embedding VECTOR(768) NOT NULL,  -- âœ… GÃ©nÃ©rÃ© par Workers Locaux
  file_path TEXT NOT NULL UNIQUE,
  upload_date TIMESTAMPTZ DEFAULT NOW(),
  metadata JSONB DEFAULT '{}'::jsonb,  -- âš ï¸ Vide (pas utilisÃ© actuellement)
  extra_data JSONB DEFAULT '{}'::jsonb,  -- âœ… UtilisÃ© par Workers
  
  CONSTRAINT unique_file_path UNIQUE (file_path)
);

-- Index pgvector pour recherche vectorielle rapide (cosine distance)
CREATE INDEX idx_documents_embedding 
ON documents 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- Index pour recherche par file_path
CREATE INDEX idx_documents_file_path ON documents(file_path);
```

**Structure extra_data (Workers Locaux) :**
```json
{
  "source": "workerlocal",
  "content_hash": "sha256...",
  "processed_at": "2025-10-08T07:22:41.152673"
}
```

**Statistiques Actuelles (12 oct 2025) :**
- **Rows** : 930,394 documents (LÃ©gifrance uniquement)
- **Embeddings** : 930,394 vecteurs (768 dims chacun)
- **Taille estimÃ©e** : ~2.8 GB (930k Ã— 768 Ã— 4 bytes)
- **Remplissage** : Workers Locaux (1 doc = 1 embedding contexte global)
- **Lecture** : Backend (recherche sÃ©mantique READ ONLY)
- **Sources** : LÃ©gifrance (actuel) + PLU (futur) + autres (futurs)

---

### **Table `document_chunks` (FUTURE)** ğŸ”®

```sql
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
  chunk_text TEXT NOT NULL,
  chunk_index INT NOT NULL,
  chunk_type TEXT,  -- 'article', 'section', 'paragraph'
  embedding VECTOR(768) NOT NULL,  -- âœ… GÃ©nÃ©rÃ© par Workers (futur)
  metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  CONSTRAINT fk_document FOREIGN KEY (document_id) 
    REFERENCES documents(id) ON DELETE CASCADE
);

-- Index pgvector pour recherche granulaire
CREATE INDEX idx_chunks_embedding 
ON document_chunks 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- Index pour recherche par document parent
CREATE INDEX idx_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_chunks_type ON document_chunks(chunk_type);
```

**Statistiques PrÃ©vues** :
- **Rows** : ~5-10 millions de chunks (estimÃ©)
- **Ratio** : 8-15 chunks par document
- **Remplissage** : Workers Locaux v2.0 (FUTUR)
- **Lecture** : Backend (recherche prÃ©cise)

---

### **Gestion Multi-Sources (12 oct 2025 - FIX)** ğŸ¯

**Principe** : Le chatbot aide les architectes sur **TOUTES** les sources de documentation disponibles.

**Sources actuelles et futures :**
| Source | Service collecteur | Valeur extra_data.source | Status | Documents |
|--------|-------------------|--------------------------|--------|-----------|
| **LÃ©gifrance** | Microservice LÃ©gifrance PISTE | `"workerlocal"` | âœ… Actif | 930,394 |
| **PLU/GPU** | Microservice Docs PLU | `"plu"` (futur) | â¸ï¸ En dev | 0 |
| **Autres** | Services futurs | Ã€ dÃ©finir | â¸ï¸ Futur | 0 |

**Backend RAG (fix 12 oct 2025) :**
```python
# âœ… APRÃˆS FIX: Recherche multi-sources (TOUS les documents)
results = await search_documents(
    query="rÃ¨gles urbanisme",
    limit=10,
    similarity_threshold=0.5
    # âœ… Pas de filtre source - Chatbot cherche dans LÃ©gifrance + PLU + autres
)

# Si besoin de filtrer par source (optionnel futur) :
results = await search_documents(
    query="rÃ¨gles urbanisme",
    filters={"source": "workerlocal"}  # Filtre spÃ©cifique LÃ©gifrance
)
```

**Avantages** :
- âœ… Chatbot rÃ©pond avec TOUTE la doc disponible (LÃ©gifrance + PLU + futurs)
- âœ… Pas besoin de reparser les 930k documents existants
- âœ… Ajout de nouvelles sources transparent (Workers futurs utilisent autre valeur source)
- âœ… Filtrage optionnel possible si besoin

---

### **Fix Version llama-cpp-python + Threshold (13 oct 2025 - FIX CRITIQUE)** ğŸ”§

**ProblÃ¨me initial** : RAG retournait **0 rÃ©sultats** malgrÃ© 930k documents indexÃ©s.

**Diagnostic approfondi** :

| Ã‰tape | ProblÃ¨me identifiÃ© | Solution appliquÃ©e |
|-------|-------------------|-------------------|
| **1** | Backend avait 7 paramÃ¨tres Llama diffÃ©rents de WorkerLocal | âœ… Retrait de `seed`, `use_mmap`, `use_mlock`, `logits_all`, `vocab_only`, `n_gpu_layers`, `numa` |
| **2** | Backend ajoutait prÃ©fixe "query :" selon doc Solon-base | âœ… Retrait prÃ©fixe (WorkerLocal n'en a pas) |
| **3** | Race condition subprocess (2+ requÃªtes parallÃ¨les) | âœ… Ajout `asyncio.Lock()` |
| **4** | Tentative de normalisation embeddings L2=1 | âœ… Retrait normalisation (brut comme WorkerLocal) |
| **5** | **Version llama-cpp flexible (`>=0.2.20`)** | âœ… **FixÃ© Ã  `==0.3.16` exactement comme WorkerLocal** |
| **6** | Threshold 0.7 trop serrÃ© (distances rÃ©elles ~0.6-0.7) | âœ… ConfirmÃ© Ã  0.70 (distances 0.67 OK) |
| **7** | Query test "test urbanisme" peu naturelle | âœ… ChangÃ© en "Quelles sont les regles urbanisme" |

**Cause racine identifiÃ©e** :

**Versions llama-cpp-python diffÃ©rentes** :
```python
# âŒ AVANT (Backend requirements.txt)
llama-cpp-python>=0.2.20  # Version flexible â†’ pouvait Ãªtre 0.2.20, 0.2.50, 0.3.0, etc.

# âœ… APRÃˆS (Backend requirements.txt)
llama-cpp-python==0.3.16  # Version EXACTE identique Ã  WorkerLocal
```

**Impact** :
- Versions diffÃ©rentes = commits `llama.cpp` sous-jacents diffÃ©rents
- Flags de compilation diffÃ©rents (Windows vs Linux)
- **Embeddings gÃ©nÃ©rÃ©s diffÃ©rents** malgrÃ© mÃªmes paramÃ¨tres
- Distances incompatibles â†’ 0 rÃ©sultats

**Solution finale (commit `66d7f5d` - 13 oct 2025)** :

```python
# Agent-Orchestrator/backend/requirements.txt
--no-binary=llama-cpp-python
llama-cpp-python==0.3.16  # âœ… VERSION EXACTE

# Agent-Orchestrator/backend/llama_server.py (ligne 41-51)
llm = llama_cpp.Llama(
    model_path=str(model_path),
    embedding=True,
    n_ctx=512,
    n_batch=2048,
    n_threads=cpu_count,
    n_ubatch=2048,
    low_vram=False,
    f16_kv=True,
    verbose=False
    # âœ… STRICTEMENT IDENTIQUE Ã  WorkerLocal (0 paramÃ¨tre diffÃ©rent)
)

# GÃ©nÃ©ration embedding (ligne 60-79)
embedding_list = embedding_response['data'][0]['embedding']
return {
    "success": True,
    "embedding": embedding_list  # âœ… Embedding BRUT (pas de normalisation)
}
```

**Tests de validation** :

```bash
# Test 1: Query naturelle
Query: "Quelles sont les regles urbanisme"
RÃ©sultats: 3 documents âœ…
Distance: 0.6774 (< 0.70) âœ…

# Test 2: Mot-clÃ© simple
Query: "urbanisme"
RÃ©sultats: 5 documents âœ…
Distance: 0.5977 (< 0.70) âœ…

# Test 3: Auto-similaritÃ© PostgreSQL (pgvector)
SELECT embedding <=> embedding AS distance_self
FROM documents LIMIT 1;
RÃ©sultat: distance_self = 0 âœ…
```

**Thresholds finaux** :
```python
# Endpoints RAG (Agent-Orchestrator/backend/api/v3/rag.py)
/search-legifrance:  similarity_threshold=0.70  # RecommandÃ© Solon-base
/documents/top:      similarity_threshold=0.70  # AlignÃ©
```

**Edge Function system-tests (v8)** :
```typescript
// Query naturelle pour test rÃ©aliste
const response = await fetch(
  `${BACKEND}/api/v3/rag/search-legifrance?query=Quelles+sont+les+regles+urbanisme&limit=3`
);
```

**Distances observÃ©es** :
- Queries naturelles : ~0.60-0.70 (plage optimale)
- Mot-clÃ© simple : ~0.55-0.65 (trÃ¨s bon)
- Auto-similaritÃ© DB : 0 (parfait)

**Conclusion** :
- âœ… Version llama-cpp-python **DOIT Ãªtre exacte** (pas `>=`)
- âœ… ParamÃ¨tres Llama **strictement identiques** Backend = Workers
- âœ… Pas de normalisation (embeddings bruts)
- âœ… Pas de prÃ©fixe "query :"
- âœ… `json.dumps()` pour pgvector
- âœ… Lock asyncio pour subprocess
- âœ… Threshold 0.70 optimal pour Solon-base

---

### **IntÃ©gration RAG dans le Chatbot (13 oct 2025 - CONNEXION COMPLÃˆTE)** ğŸ’¬

**Objectif** : Le chatbot utilise automatiquement la recherche sÃ©mantique pour enrichir ses rÃ©ponses.

**ImplÃ©mentation dans l'Orchestrator** :

**Fichier** : `Agent-Orchestrator/backend/agents/orchestrator.py` (ligne 286-360)

```python
async def process_message_stream(
    self,
    user_id: str,
    session_id: str,
    messages: list,
    project_id: str | None = None,
    model: str | None = None,
) -> AsyncGenerator[str, None]:
    """
    Traite un message utilisateur en streaming avec recherche sÃ©mantique directe.
    """
    # ... (initialisation LLM, MCP) ...
    
    # ğŸ” RECHERCHE SÃ‰MANTIQUE AUTOMATIQUE
    semantic_context = ""
    try:
        # RÃ©cupÃ©rer la derniÃ¨re question utilisateur
        user_question = ""
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                user_question = msg.content
                break
        
        if user_question:
            # âœ… Recherche multi-sources avec threshold 0.70
            search_request = SearchRequest(
                query=user_question,
                mode=SearchMode.SEMANTIC,
                limit=8,  # 8 documents recherchÃ©s
                threshold=0.70,  # âœ… FIX: Threshold optimal
                filters=None  # âœ… FIX: Multi-sources (LÃ©gifrance + PLU + futurs)
            )
            
            search_response = await search_service.search(search_request)
            
            if search_response.results:
                context_docs = []
                legal_references = []
                
                # Top 5 rÃ©sultats utilisÃ©s
                for i, result in enumerate(search_response.results[:5]):
                    content = result.content[:400]  # 400 chars max
                    
                    # Extraction rÃ©fÃ©rences lÃ©gales automatique
                    # (Articles, Codes, DÃ©crets, ArrÃªtÃ©s)
                    legal_patterns = [
                        r'art\.?\s*L?\d+[-\w]*',
                        r'Code\s+[A-Z][a-z]+',
                        r'DÃ©cret\s+nÂ°?\s*\d+',
                        r'ArrÃªtÃ©\s+nÂ°?\s*\d+'
                    ]
                    
                    found_refs = []
                    for pattern in legal_patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        found_refs.extend(matches)
                    
                    context_docs.append(f"{i+1}. {content}...")
                    legal_references.extend(found_refs)
                
                # Construction du contexte documentaire
                semantic_context = f"""
                
CONTEXTE DOCUMENTAIRE (Recherche sÃ©mantique sur "{user_question[:100]}..."):

{chr(10).join(context_docs)}

RÃ©fÃ©rences lÃ©gales identifiÃ©es : {', '.join(set(legal_references[:10]))}
                """
                
                logger.info(f"ğŸ“š Contexte documentaire construit: {len(context_docs)} docs, {len(set(legal_references))} rÃ©fÃ©rences")
    
    except Exception as e:
        logger.warning(f"âš ï¸ Recherche sÃ©mantique Ã©chouÃ©e: {e}")
        # Continuer sans contexte documentaire
    
    # Injection du contexte dans le system prompt
    system_prompt = f"""Tu es ArchiReg, assistant expert en urbanisme franÃ§ais.
    
{semantic_context}

EXPERTISE MÃ‰TIER :
- DonnÃ©es provenant du micro-service LÃ©gifrance PISTE (16 codes juridiques)
- RÃ©fÃ©rences juridiques prÃ©cises
...
"""
    
    # Envoyer au LLM Groq avec contexte enrichi
    # ... (streaming vers frontend)
```

**Flow complet** :

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant F as Frontend
    participant O as Orchestrator
    participant R as RAG Service
    participant DB as pgvector
    participant LLM as Groq LLM
    
    U->>F: "Quelles sont les rÃ¨gles urbanisme ?"
    F->>O: /api/v3/chat/completions
    O->>R: Recherche sÃ©mantique automatique
    R->>DB: Embedding query + recherche vectorielle
    DB-->>R: 8 documents pertinents (distance < 0.70)
    R-->>O: Top 5 documents + rÃ©fÃ©rences lÃ©gales
    O->>O: Injection contexte dans system prompt
    O->>LLM: Messages + contexte documentaire
    LLM-->>O: RÃ©ponse enrichie (streaming)
    O-->>F: Streaming chunks
    F-->>U: RÃ©ponse avec rÃ©fÃ©rences LÃ©gifrance âœ…
```

**Avantages** :
- âœ… Recherche automatique (pas besoin d'activer manuellement)
- âœ… Multi-sources (LÃ©gifrance + PLU + futurs)
- âœ… Top 5 docs les plus pertinents
- âœ… Extraction automatique rÃ©fÃ©rences lÃ©gales
- âœ… Contexte injectÃ© dans chaque rÃ©ponse
- âœ… RÃ©ponses enrichies avec articles de loi prÃ©cis

**Logs Render attendus** :
```
ğŸ” DÃ‰BUT RECHERCHE SÃ‰MANTIQUE - Question: ...
ğŸ” Initialisation du service de recherche...
ğŸ” ExÃ©cution de la recherche sÃ©mantique...
ğŸ” RÃ©sultats de recherche: 8 documents
ğŸ“š Contexte documentaire construit: 5 docs, X rÃ©fÃ©rences
```

**Tests** :
- âœ… Test systÃ¨me RAG : 3 documents trouvÃ©s
- âœ… Chatbot frontend : RÃ©ponses avec contexte LÃ©gifrance

---

### **Lazy Loading & Performance (13 oct 2025)** âš¡

**Principe** : Le modÃ¨le GGUF (500 MB) se charge **Ã  la premiÃ¨re utilisation** pour Ã©conomiser la RAM au dÃ©marrage.

**Comportement** :

| Ã‰vÃ©nement | Action | Temps |
|-----------|--------|-------|
| **DÃ©marrage Backend** | Initialisation services SANS charger le modÃ¨le | ~5s |
| **PremiÃ¨re requÃªte RAG** | TÃ©lÃ©chargement + Chargement modÃ¨le GGUF | ~13-15s â³ |
| **RequÃªtes suivantes** | ModÃ¨le dÃ©jÃ  chargÃ© en mÃ©moire | ~1-2s âœ… |

**ImplÃ©mentation** :

```python
# services/supabase_embedding_service.py (ligne 56-70)
async def initialize(self):
    """Initialisation lazy du service"""
    if self._initialized:
        return
    
    logger.info("ğŸ”„ Initialisation lazy GGUFEmbeddingService...")
    
    # 1. TÃ©lÃ©charger modÃ¨le si nÃ©cessaire (~2s)
    await self._download_model_if_needed()
    
    # 2. Lancer subprocess llama-cpp (~13s premiÃ¨re fois)
    await self._start_llama_subprocess()
    
    self._initialized = True
    logger.info("âœ… GGUFEmbeddingService prÃªt!")
```

**Avantages** :
- âœ… DÃ©marrage backend rapide (pas de timeout Render)
- âœ… RAM Ã©conomisÃ©e si service pas utilisÃ©
- âœ… Model partagÃ© entre toutes les requÃªtes

**InconvÃ©nient** :
- â³ PremiÃ¨re requÃªte lente (~15s)
- Solution : Warm-up endpoint (optionnel futur)

**Logs** :
```
[info] ğŸ”„ Initialisation lazy du service embedding...
[info] ğŸ“¥ TÃ©lÃ©chargement modÃ¨le GGUF...
[info] ğŸ”„ Chargement modÃ¨le llama-cpp via subprocess...
[info] âœ… ModÃ¨le llama-cpp chargÃ© avec succÃ¨s
[info] âœ… Subprocess confirmÃ© ready!
```

---

### **Lock Asyncio Subprocess (13 oct 2025 - FIX RACE CONDITION)** ğŸ”’

**ProblÃ¨me** : Si 2+ requÃªtes arrivent simultanÃ©ment, elles tentent de lire le subprocess en parallÃ¨le â†’ **CRASH** !

**Erreur observÃ©e** :
```
readuntil() called while another coroutine is already waiting for incoming data
```

**Solution** : Lock asyncio pour synchroniser l'accÃ¨s au subprocess.

**ImplÃ©mentation** :

```python
# services/supabase_embedding_service.py (ligne 49-54)
class GGUFEmbeddingService:
    def __init__(self):
        # ... autres attributs ...
        self._subprocess_lock = asyncio.Lock()  # âœ… FIX: Lock pour Ã©viter race condition

# services/supabase_embedding_service.py (ligne 244-274)
async def _call_llama_subprocess(self, text: str) -> list[float]:
    """Appelle le subprocess llama-cpp pour gÃ©nÃ©rer un embedding"""
    
    # âœ… FIX: Lock garantit 1 seule requÃªte Ã  la fois
    async with self._subprocess_lock:
        try:
            # PrÃ©parer requÃªte JSON
            request = {"text": text}
            request_json = json.dumps(request) + "\n"
            
            # Envoyer au subprocess
            self._llama_process.stdin.write(request_json.encode())
            await self._llama_process.stdin.drain()
            
            # Lire rÃ©ponse (atomique, pas de race condition)
            response_line = await self._llama_process.stdout.readline()
            response = json.loads(response_line.decode().strip())
            
            return response["embedding"]
        
        except Exception as e:
            logger.error(f"âŒ Erreur communication subprocess: {e}")
            raise
```

**Pourquoi nÃ©cessaire** :
- Subprocess = 1 seul processus partagÃ©
- stdin/stdout = 1 seule pipe
- Si 2 requÃªtes lisent en parallÃ¨le â†’ mÃ©lange des rÃ©ponses âŒ
- Lock garantit ordre sÃ©quentiel âœ…

**Impact performance** :
- Minimal (~0-50ms latency ajoutÃ©e)
- Ã‰vite crashes critiques
- Garantit fiabilitÃ© 100%

---

### **Table `parsed_files` (Anti-duplication)** âœ…

```sql
CREATE TABLE parsed_files (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_path TEXT NOT NULL UNIQUE,
  content_hash TEXT NOT NULL,
  worker_id TEXT NOT NULL,
  status TEXT DEFAULT 'completed',  -- 'completed', 'failed'
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  CONSTRAINT unique_file_path UNIQUE (file_path)
);

-- Index pour vÃ©rification rapide des doublons
CREATE INDEX idx_parsed_files_path ON parsed_files(file_path);
CREATE INDEX idx_parsed_files_hash ON parsed_files(content_hash);
```

**RÃ´le** : Ã‰viter de reparser les mÃªmes fichiers

**Statistiques (12 oct 2025)** :
- **Rows** : 930,937 fichiers parsÃ©s
- **Usage** : Anti-duplication 99.9% efficace

---

## ğŸ§  ModÃ¨le GGUF : SOLON-EMBEDDINGS-BASE

### **CaractÃ©ristiques**

| PropriÃ©tÃ© | Valeur |
|-----------|--------|
| **Nom** | Solon-embeddings-base-0.1.Q8_0.gguf |
| **Dimensions** | 768 |
| **Taille** | ~500 MB |
| **Quantization** | Q8_0 (8-bit) |
| **Langue** | FranÃ§ais (optimisÃ©) |
| **SpÃ©cialisation** | Textes juridiques franÃ§ais |
| **Format** | GGUF (llama.cpp compatible) |

### **Stockage**

**Bucket Supabase** :
- **Nom** : `ai-models`
- **Fichier** : `Solon-embeddings-base-0.1.Q8_0.gguf`
- **URL publique** : `https://joozqsjbcwrqyeqepnev.supabase.co/storage/v1/object/public/ai-models/Solon-embeddings-base-0.1.Q8_0.gguf`

**Cache Local** :
- **Backend** : `Agent-Orchestrator/backend/cache/shared/`
- **Worker** : `WorkerLocal/models/`

### **Chargement Backend (Lazy Loading)**

```python
# services/supabase_embedding_service.py
class GGUFEmbeddingService:
    async def _download_model_if_needed(self):
        """TÃ©lÃ©charge le modÃ¨le si absent localement"""
        if not self.local_cache_path.exists():
            logger.info("ğŸ“¥ TÃ©lÃ©chargement modÃ¨le GGUF...")
            async with aiohttp.ClientSession() as session:
                async with session.get(self.supabase_storage_url) as response:
                    if response.status == 200:
                        with open(self.local_cache_path, "wb") as f:
                            async for chunk in response.content.iter_chunked(8192):
                                f.write(chunk)
                        logger.info("âœ… ModÃ¨le tÃ©lÃ©chargÃ©")
```

### **Chargement Worker (Startup)**

```python
# embedding/llama_service.py
async def _load_model(self):
    """Charge le modÃ¨le llama-cpp"""
    self.llm = Llama(
        model_path=str(self.model_path),
        embedding=True,
        n_ctx=512,               # âœ… FIX: AlignÃ© avec n_ctx_train (qualitÃ© 100%)
        n_batch=2048,            # Batch MASSIF
        n_threads=cpu_count,     # Tous les cores
        n_ubatch=2048,           # Batch unitaire
        low_vram=False,          # Utiliser toute la RAM
        f16_kv=True,             # Float16 pour perfs
        verbose=False
    )
```

### **Chargement Backend (Subprocess llama_server.py)**

```python
# llama_server.py
def load_llama_model():
    """Charge le modÃ¨le llama-cpp avec paramÃ¨tres optimisÃ©s pour Render"""
    llm = llama_cpp.Llama(
        model_path=str(model_path),
        embedding=True,  # MODE EMBEDDING OBLIGATOIRE
        n_ctx=512,       # âœ… FIX: ALIGNÃ‰ avec n_ctx_train (qualitÃ© 100%)
        n_batch=2048,    # âœ… Batch MASSIF pour performance
        n_threads=cpu_count,  # âœ… TOUS les cores
        n_ubatch=2048,   # âœ… Micro-batch MASSIF
        verbose=False,   # Pas de logs verbeux
        # OPTIMISATIONS RENDER 2GB RAM :
        n_gpu_layers=0,  # Force CPU (pas GPU sur Render)
        use_mmap=True,   # Memory mapping efficace
        use_mlock=False, # Pas de memory lock (problÃ©matique conteneurs)
        f16_kv=True,     # âœ… Float16 pour performance
        logits_all=False, # Pas besoin logits pour embeddings
        vocab_only=False, # Chargement modÃ¨le complet nÃ©cessaire
        seed=-1,         # Seed alÃ©atoire
        low_vram=False,  # âœ… DÃ©sactiver mode restrictif
        numa=False,      # DÃ©sactiver NUMA
    )
    return llm
```

**âš ï¸ IMPORTANT FIX n_ctx** :
- âœ… **Worker** : `n_ctx=512` (alignÃ© avec n_ctx_train)
- âœ… **Backend** : `n_ctx=512` (alignÃ© avec n_ctx_train)
- âœ… **ModÃ¨le** : `n_ctx_train=512` (bert.context_length dans metadata GGUF)
- âœ… **QualitÃ©** : 100% (pas de warning, pas de truncation)

**ProblÃ¨me rÃ©solu** :
- âŒ **Avant** : `n_ctx=4096` â†’ Warning "exceeds trained context length" â†’ QualitÃ© dÃ©gradÃ©e
- âœ… **AprÃ¨s** : `n_ctx=512` â†’ Pas de warning â†’ QualitÃ© 100%

**Performance Backend** :
- âš¡ **PremiÃ¨re requÃªte** : ~500ms (warm-up subprocess)
- âš¡ **RequÃªtes suivantes** : ~15-20ms (subprocess chaud)
- âš¡ **Warm-up automatique** : `get_embedding("test warm-up")` au dÃ©marrage

---

## ğŸ—ºï¸ Diagramme Flux Complet (Tables Supabase)

```mermaid
graph TB
    subgraph "COLLECTE"
        MS[Microservice LÃ©gifrance] -->|1. Upload JSON| BKT[Bucket: agentbasic-legifrance-raw]
        MS -->|2. INSERT status=pending| FQ[Table: files_queue]
    end
    
    subgraph "TRAITEMENT NIVEAU 1: Contexte Global"
        W1[Worker Local 1] -->|3a. SELECT pending| FQ
        W2[Worker Local 2] -->|3b. SELECT pending| FQ
        W3[Worker Local 3] -->|3c. SELECT pending| FQ
        W1 -->|4a. Download JSON| BKT
        W2 -->|4b. Download JSON| BKT
        W3 -->|4c. Download JSON| BKT
        W1 -->|5a. INSERT hash| PF[Table: parsed_files]
        W2 -->|5b. INSERT hash| PF
        W3 -->|5c. INSERT hash| PF
        W1 -->|6a. INSERT + embedding| DOCS[Table: documents<br/>embedding VECTOR 768<br/>extra_data source=workerlocal]
        W2 -->|6b. INSERT + embedding| DOCS
        W3 -->|6c. INSERT + embedding| DOCS
        W1 -->|7a. UPDATE completed| FQ
        W2 -->|7b. UPDATE completed| FQ
        W3 -->|7c. UPDATE completed| FQ
    end
    
    subgraph "TRAITEMENT NIVEAU 2: Chunks Granulaires FUTUR"
        WC1[WorkerLocal Chunk 1] -.->|8. SELECT pending chunks| FQ
        WC1 -.->|9. Download JSON| BKT
        WC1 -.->|10. SELECT parent doc| DOCS
        WC1 -.->|11. INSERT chunks + embeddings| CHK[Table: document_chunks<br/>FUTUR]
    end
    
    subgraph "RECHERCHE SÃ‰MANTIQUE"
        FE[Frontend Chat] -->|12. Query user| BE[Backend Agent-Orchestrator]
        BE -->|13. GÃ©nÃ¨re embedding query| GGUF[GGUF Service<br/>READ ONLY]
        BE -->|14. SELECT vectoriel<br/>embedding vector_cosine_ops<br/>SANS filtre source| DOCS
        BE -.->|15. SELECT vectoriel chunks FUTUR| CHK
        BE -->|16. Retour rÃ©sultats JSON| FE
    end
    
    style MS fill:#f59e0b,stroke:#000,stroke-width:2px
    style W1 fill:#a78bfa,stroke:#000
    style W2 fill:#a78bfa,stroke:#000
    style W3 fill:#a78bfa,stroke:#000
    style WC1 fill:#f472b6,stroke:#000,stroke-dasharray: 5 5
    style BE fill:#60a5fa,stroke:#000,stroke-width:2px
    style DOCS fill:#ef4444,stroke:#000,stroke-width:3px
    style CHK fill:#ef4444,stroke:#000,stroke-dasharray: 5 5
    style BKT fill:#10b981,stroke:#000
    style FQ fill:#facc15,stroke:#000
    style PF fill:#facc15,stroke:#000
```

### **RÃ´les des tables Supabase**

| Table | RÃ´le | Ã‰crite par | Lue par | Rows (12 oct) |
|-------|------|------------|---------|---------------|
| **files_queue** | Queue traitement + status | Microservice | Workers | 1,034,780 |
| **parsed_files** | Anti-duplication (hash) | Workers | Workers (vÃ©rif doublon) | 930,937 |
| **documents** | Documents + embeddings globaux | Workers | Backend (RAG) + Workers Chunk | **930,394** |
| **document_chunks** | Chunks + embeddings prÃ©cis | Workers Chunk (futur) | Backend (RAG futur) | 0 (prÃªt) |

### **Colonnes critiques `documents`**

| Colonne | Type | RÃ´le | Remplie par | Exemple |
|---------|------|------|-------------|---------|
| `id` | UUID | ClÃ© primaire | Supabase auto | `uuid-123...` |
| `title` | TEXT | Titre document | Workers | "Code de l'urbanisme" |
| `content` | TEXT | Texte complet | Workers | Contenu JSON parsÃ© |
| `embedding` | VECTOR(768) | Vecteur sÃ©mantique | Workers (GGUF) | `[0.123, -0.456, ...]` |
| `file_path` | TEXT UNIQUE | Chemin source | Workers | `legifrance/.../xxx.json` |
| `metadata` | JSONB | MÃ©tadonnÃ©es (vide) | - | `{}` (pas utilisÃ©) |
| `extra_data` | JSONB | TraÃ§abilitÃ© source | Workers | `{"source": "workerlocal", ...}` |

**Index pgvector actif :**
```sql
CREATE INDEX idx_documents_embedding 
ON documents 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);
-- âœ… Recherche vectorielle en ~15-20ms sur 930k docs
```

---

## ğŸ”„ Flux de DonnÃ©es Complet

### **Phase 1 : Collecte** ğŸ“¡

```
Micro-service LÃ©gifrance PISTE
    â†“ [API PISTE - Codes, DÃ©crets, Textes]
Fichiers JSON bruts (titre, contenu, articles, sections)
    â†“ [Upload HTTP]
Bucket Supabase Storage (agentbasic-legifrance-raw)
    â””â”€ legifrance/architecture_urbanisme/
       â”œâ”€ code_urbanisme_123.json
       â”œâ”€ decret_456.json
       â””â”€ ... (1,077,264 fichiers)
```

**MÃ©triques** :
- **Fichiers collectÃ©s** : 1,077,264
- **Taille totale** : 4.6 GB
- **Format** : JSON structurÃ©

---

### **Phase 2 : Traitement** ğŸ­

```
Bucket Supabase Storage
    â†“ [API Storage - TÃ©lÃ©chargement]
Worker Local (rÃ©cupÃ¨re batch de 100)
    â†“ [Parse JSON - Extraction texte]
Texte complet (titre + contenu + articles)
    â†“ [llama-cpp-python - GÃ©nÃ©ration embedding]
Embedding GGUF (768 dimensions)
    â†“ [INSERT SQL]
Table documents (avec colonne embedding vector(768))
    â†“ [Indexation automatique]
pgvector (index ivfflat - 100 listes)
```

**MÃ©triques (12 oct 2025)** :
- **Documents parsÃ©s** : 930,394 (LÃ©gifrance)
- **Embeddings gÃ©nÃ©rÃ©s** : 930,394 vecteurs (768 dims)
- **Performance** : 50 fichiers/sec
- **Anti-duplication** : 99.9%
- **Sources** : LÃ©gifrance (actuel), PLU (futur)

---

### **Phase 3 : Recherche** ğŸ”

```
Frontend (query utilisateur: "rÃ¨gles urbanisme")
    â†“ [HTTP GET]
Backend /api/v3/rag/search-legifrance
    â†“ [GGUFEmbeddingService]
GÃ©nÃ©ration embedding query (768 dims)
    â†“ [SupabaseSearchService]
Recherche vectorielle pgvector
    â†“ [SQL - Distance cosinus]
SELECT ... ORDER BY embedding <-> query_vector
    â†“ [Top K rÃ©sultats - distance < 0.3]
10 documents les plus pertinents
    â†“ [HTTP Response JSON]
Frontend (affichage rÃ©sultats)
```

**MÃ©triques** :
- **Latence embedding query** : ~15ms
- **Latence recherche pgvector** : ~5ms
- **Latence totale** : ~20-30ms
- **Cache hits** : 60-70%

---

## ğŸ“Š MÃ©triques & Performance

### **Workers Locaux**

| MÃ©trique | Valeur | Notes |
|----------|--------|-------|
| **Vitesse traitement** | 50 fichiers/sec | Concurrence 50 threads |
| **Embedding time** | ~20ms/doc | ModÃ¨le Q8_0 optimisÃ© |
| **Batch size** | 100 fichiers | Optimal pour pagination |
| **Timeout download** | 30s | Fichiers bucket Supabase |
| **Timeout processing** | 30s | Parsing + embedding |
| **Anti-duplication** | 99.9% | Via content_hash SHA256 |

### **Backend RAG**

| MÃ©trique | Valeur | Notes |
|----------|--------|-------|
| **Embedding query** | ~15-20ms | GÃ©nÃ©ration via GGUF subprocess |
| **Recherche pgvector** | ~5ms | Index ivfflat (100 listes) |
| **Latence totale** | ~20-30ms | End-to-end |
| **Cache hits** | 60-70% | Redis embedding cache |
| **Seuil distance** | <0.3 | RÃ©sultats pertinents |
| **PremiÃ¨re requÃªte** | ~500ms | Warm-up subprocess llama.cpp |
| **n_ctx** | 512 | âœ… AlignÃ© avec n_ctx_train |

### **Base de DonnÃ©es**

| MÃ©trique | Valeur | Notes |
|----------|--------|-------|
| **Documents indexÃ©s** | 930,394 | Table documents (LÃ©gifrance + futurs) |
| **Taille embeddings** | ~2.8 GB | 930k Ã— 768 Ã— 4 bytes |
| **Index pgvector** | HNSW | m=16, ef_construction=64, 1.4GB |
| **Distance metric** | Cosinus | Optimal pour embeddings |
| **Threshold** | 0.70 | Seuil de similaritÃ© (1-distance) |
| **ef_search** | 100 | PrÃ©cision/vitesse optimale |
| **statement_timeout** | 60s | Timeout pour requÃªtes lourdes |

### **Index HNSW (DÃ©tails Techniques)**

| Table | Index | Taille | Rows | Usage | Statut |
|-------|-------|--------|------|-------|--------|
| **documents** | `idx_documents_embedding_hnsw` | 1.4 GB | 930,701 | Contexte global des codes juridiques | âœ… ACTIF |
| **document_chunks** | `idx_document_chunks_embedding_hnsw` | 16 kB | 0 | DÃ©tails granulaires (articles, paragraphes) | â³ FUTUR |

**ğŸ”® StratÃ©gie Hybride Future** :
- **Recherche large** : Index `documents` pour trouver codes pertinents (<500ms)
- **Recherche prÃ©cise** : Index `document_chunks` pour article exact (<300ms)
- **Fusion intelligente** : Backend combine les 2 rÃ©sultats (<800ms total)
- **Auto-maintenance** : Les 2 index HNSW s'adaptent automatiquement, aucun cron job requis

---

## âœ… Tests SystÃ¨me

### **Test RAG** ğŸ§ª

**Localisation** : `DOCS-ARCHITECTURE/05-EDGE-FUNCTIONS/system-tests/index.ts`

**Fonction** :
```typescript
async function testRAG(userToken: string): Promise<TestResult> {
  // âœ… Appelle le backend directement (pas d'Edge Function rag-endpoint)
  const response = await fetch(
    `${SERVICES.backend}/api/v3/rag/search-legifrance?query=test+urbanisme&limit=3`,
    {
      headers: { 
        'Authorization': `Bearer ${userToken}`,
        'Content-Type': 'application/json' 
      }
    }
  )
  
  // Validation rÃ©ponse
  const data = await response.json()
  const isSuccess = response.ok && data.success && data.results?.length > 0
  
  return {
    name: 'RAG Search',
    status: isSuccess ? 'success' : 'error',
    message: isSuccess 
      ? `RAG OK: ${data.results.length} documents trouvÃ©s` 
      : `RAG KO: ${data.error}`
  }
}
```

**Validation** :
- âœ… Backend accessible
- âœ… GÃ©nÃ©ration embedding query fonctionnelle
- âœ… Recherche pgvector opÃ©rationnelle
- âœ… Retour rÃ©sultats corrects

---

## ğŸš¨ Notes Importantes

### **SÃ©paration des ResponsabilitÃ©s**

| Service | RÃ´le | Embeddings |
|---------|------|------------|
| **Workers** | WRITE | âœ… GÃ©nÃ¨re embeddings documents + INSERT |
| **Backend** | READ | âœ… GÃ©nÃ¨re embeddings queries + SEARCH |
| **Micro-service** | COLLECT | âŒ Aucun embedding |

### **Source de DonnÃ©es**

- âœ… **Bucket LÃ©gifrance** = Source unique pour Workers ET Future Chunking
- âœ… Pas de duplication de source
- âœ… Anti-duplication via `parsed_files` (content_hash)

### **ModÃ¨le GGUF**

- âœ… **1 seul modÃ¨le** : Solon-embeddings-base-0.1.Q8_0.gguf
- âœ… **PartagÃ©** entre Workers et Backend
- âœ… **StockÃ©** dans bucket Supabase `ai-models`
- âœ… **TÃ©lÃ©chargÃ©** automatiquement si absent

---

## ğŸ”® Ã‰volution Future (v2.0)

### **Chunking Granulaire & Recherche Hybride Multi-Index** ğŸ¯

**Architecture Multi-Index HNSW** :

| Composant | Table | Index HNSW | Taille | Rows | Statut |
|-----------|-------|------------|--------|------|--------|
| **Niveau 1** | `documents` | `idx_documents_embedding_hnsw` | 1.4 GB | 930,701 | âœ… ACTIF |
| **Niveau 2** | `document_chunks` | `idx_document_chunks_embedding_hnsw` | 16 kB | 0 | â³ FUTUR |

**Flux de Recherche Hybride (Future v3.0)** :

```
Question utilisateur: "RÃ¨gles zone UA urbanisme?"
                â”‚
                v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Backend gÃ©nÃ¨re embedding  â”‚
    â”‚ (GGUF Solon 768 dims)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                â”‚
       v                v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚documents â”‚      â”‚document_chunksâ”‚
â”‚ 930k docsâ”‚      â”‚ ~3M chunks   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                   â”‚
     v                   v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HNSW 1.4GB  â”‚   â”‚ HNSW ~4GB    â”‚
â”‚ <500ms      â”‚   â”‚ <300ms       â”‚
â”‚ 5 codes     â”‚   â”‚ 10 articles  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FUSION INTELLIGENTE  â”‚
    â”‚ - Contexte: 5 codes  â”‚
    â”‚ - DÃ©tail: 10 articlesâ”‚
    â”‚ Total: <800ms        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM enrichi RAG      â”‚
    â”‚ RÃ©ponse ultra-prÃ©ciseâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Objectifs WorkerLocal Chunk (v2.0)** :
1. â¸ï¸ DÃ©coupage documents en chunks granulaires (articles, sections)
2. â¸ï¸ GÃ©nÃ©ration embeddings par chunk via GGUF Solon
3. â¸ï¸ Insertion dans `document_chunks` avec `document_id` parent
4. â¸ï¸ Index HNSW s'agrandit automatiquement (aucun cron job requis)

**Avantages Recherche Hybride** :
- ğŸ¯ **PrÃ©cision** : Article exact au lieu de doc entier
- ğŸ¯ **Contexte** : Vue d'ensemble du code juridique
- ğŸ¯ **Performance** : 2 index HNSW en parallÃ¨le (<800ms)
- ğŸ¯ **Auto-maintenance** : Les 2 index s'adaptent automatiquement

### **AmÃ©lioration ModÃ¨le** ğŸš€

**Pistes d'amÃ©lioration** :
1. â¸ï¸ Test Solon-embeddings-large (1024 dims)
2. â¸ï¸ Fine-tuning sur corpus juridique ArchiReg
3. â¸ï¸ Ã‰valuation benchmarks (accuracy, recall@k, MRR)
4. â¸ï¸ Comparaison modÃ¨les (multilingual-e5, BGE-M3, etc.)

---

## ğŸ“ Support Technique

**Contacts** :
- **Projet** : ArchiReg - Assistant IA pour architectes
- **Stack** : Next.js, FastAPI, Supabase, llama-cpp-python
- **Version** : 4.0.0 FINALE

**Ressources** :
- [Documentation Supabase pgvector](https://supabase.com/docs/guides/ai/vector-embeddings)
- [Documentation llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [Solon Embeddings](https://huggingface.co/OrdalieTech/Solon-embeddings-base-0.1)

---

**ğŸ“… DerniÃ¨re mise Ã  jour** : 11 octobre 2025 19:00 UTC  
**âœ… Status** : ARCHITECTURE DOCUMENTÃ‰E + FIX n_ctx + WARM-UP VALIDÃ‰S âœ…

---

## ğŸ”§ CHANGELOG v2.5

### **13 octobre 2025 18:00 UTC - Optimisation HNSW + Multi-Index Future**

**Optimisations appliquÃ©es** :
1. âœ… `supabase_client.py` : `statement_timeout` 30s â†’ 60s
2. âœ… `supabase_client.py` : Ajout `SET hnsw.ef_search = 100` (prÃ©cision/vitesse optimale)
3. âœ… `supabase_search_service.py` : Logs dÃ©taillÃ©s (temps embedding, recherche, total)
4. âœ… Documentation : Architecture multi-index HNSW future (documents + document_chunks)

**Index HNSW crÃ©Ã©s** :
- âœ… `idx_documents_embedding_hnsw` : 1.4 GB, m=16, ef_construction=64, 930k docs
- âœ… `idx_document_chunks_embedding_hnsw` : 16 kB, vide (prÃªt pour chunking futur)

**RÃ©sultats attendus** :
- ğŸ¯ Recherche sÃ©mantique : 30s timeout â†’ <1s
- ğŸ¯ PrÃ©cision optimale : ef_search=100 (recommandÃ© Supabase)
- ğŸ¯ StratÃ©gie hybride future : 2 index HNSW en parallÃ¨le (<800ms)

**Maintenance** :
- âœ… Auto-maintenance : Index HNSW s'adapte automatiquement
- âœ… Aucun cron job requis : VACUUM ANALYZE hebdo suffit (jobid 3)

---

### **11 octobre 2025 - FIX n_ctx + Warm-up**

**ProblÃ¨me identifiÃ©** :
- âŒ Backend : `n_ctx=4096` â†’ Warning "exceeds trained context length 512"
- âŒ WorkerLocal : `n_ctx=4096` â†’ Warning identique
- âŒ PremiÃ¨re requÃªte RAG : 4.45s (subprocess lazy loading)

**Corrections appliquÃ©es** :
1. âœ… `backend/llama_server.py` : `n_ctx=512` (alignÃ© avec modÃ¨le)
2. âœ… `WorkerLocal/embedding/llama_service.py` : `n_ctx=512` (alignÃ© avec modÃ¨le)
3. âœ… `backend/core/startup.py` : Warm-up automatique `get_embedding("test warm-up")`
4. âœ… `backend/Dockerfile` : DÃ©sactivation AVX2/FMA pour compatibilitÃ© AMD EPYC Render

**RÃ©sultats** :
- âœ… **QualitÃ© embeddings** : 100% (pas de warning, pas de truncation)
- âœ… **Latence 1Ã¨re requÃªte** : 4.45s â†’ ~500ms (-89%)
- âœ… **Backend SIGILL** : RÃ©solu (compilation CPU gÃ©nÃ©rique SSE4)
- âœ… **Worker mode** : `adaptive` â†’ `read_only`

**Validation** :
```sql
SELECT COUNT(*) as docs_over_512_tokens 
FROM documents 
WHERE LENGTH(content) > 2048;
-- RÃ©sultat: 0 (tous les docs < 512 tokens, pas besoin de reparser)
```

---

## ğŸ“Š Index HNSW : Explication ComplÃ¨te (13 oct 2025)

### ğŸ¯ Ã€ Quoi Sert l'Index HNSW ?

**HNSW = Hierarchical Navigable Small World**

C'est un **catalogue intelligent** qui permet de trouver rapidement des documents similaires dans une base massive.

#### ğŸ“š Analogie Simple : La BibliothÃ¨que

**TA BASE DE DONNÃ‰ES = UNE BIBLIOTHÃˆQUE DE 930,000 LIVRES**

**ğŸ”´ SANS INDEX (recherche linÃ©aire) :**

```
Utilisateur : "Trouve-moi des livres sur l'urbanisme"

BibliothÃ©caire (sans catalogue) :
  ğŸ“– Livre 1 : "Cuisine franÃ§aise" â†’ Compare â†’ Distance = 0.95 âŒ
  ğŸ“– Livre 2 : "Histoire de Paris" â†’ Compare â†’ Distance = 0.82 âŒ
  ğŸ“– Livre 3 : "Code civil" â†’ Compare â†’ Distance = 0.74 âŒ
  ... (930,000 comparaisons)
  ğŸ“– Livre 500,000 : "Urbanisme Lyon" â†’ Compare â†’ Distance = 0.45 âœ…
  ... (430,000 comparaisons restantes)
  ğŸ“– Livre 930,000 : "Cuisine italienne" â†’ Compare â†’ Distance = 0.98 âŒ

â±ï¸ Temps : 30+ secondes â†’ TIMEOUT âŒ
ğŸ“Š Calculs : 930,000 comparaisons (une par document)
ğŸ¯ RÃ©sultat : Impossible Ã  utiliser en production
```

**ğŸŸ¢ AVEC INDEX HNSW (recherche intelligente) :**

```
Utilisateur : "Trouve-moi des livres sur l'urbanisme"

BibliothÃ©caire (avec catalogue HNSW) :
  ğŸ“Š Consulte le CATALOGUE INTELLIGENT
     â†’ Niveau 3 : Identifie section "Droit & Urbanisme" (10 sauts)
     â†’ Niveau 2 : Affine vers "Code urbanisme" (30 sauts)  
     â†’ Niveau 1 : Recherche fine dans candidats (100 comparaisons)
  
  ğŸ“– Livre 156,234 : "Urbanisme Lyon" â†’ Distance = 0.45 âœ…
  ğŸ“– Livre 234,567 : "Code urbanisme" â†’ Distance = 0.38 âœ…
  ğŸ“– Livre 789,123 : "PLU Paris" â†’ Distance = 0.52 âœ…

â±ï¸ Temps : <1 seconde âœ…
ğŸ“Š Calculs : ~150 comparaisons (au lieu de 930,000)
ğŸ¯ Gain : 6,200x moins de calculs !
```

### ğŸ”¬ Comment Ã‡a Marche Techniquement ?

**HNSW construit un GRAPHE MULTI-NIVEAUX :**

```mermaid
graph TB
    subgraph "NIVEAU 3 - EntrÃ©e Rapide (10 nÅ“uds)"
        A[Doc A<br/>Urbanisme] <==> B[Doc B<br/>Construction]
        B <==> C[Doc C<br/>PLU]
    end
    
    subgraph "NIVEAU 2 - Navigation (100 nÅ“uds)"
        A --> D[Doc D<br/>Permis]
        A --> E[Doc E<br/>Zonage]
        B --> F[Doc F<br/>Code civil]
        C --> G[Doc G<br/>RÃ¨glement]
    end
    
    subgraph "NIVEAU 1 - Recherche Fine (930k nÅ“uds)"
        D --> H[Doc H]
        D --> I[Doc I]
        E --> J[Doc J]
        E --> K[Doc K]
        F --> L[Doc L]
        G --> M[Doc M]
        G --> N[Doc N]
    end
    
    style A fill:#4CAF50
    style D fill:#8BC34A
    style H fill:#CDDC39
```

**PHASES DE RECHERCHE :**

1. **Niveau 3 (EntrÃ©e rapide)** : ~10 sauts pour trouver la zone gÃ©nÃ©rale
2. **Niveau 2 (Navigation)** : ~30 sauts pour affiner
3. **Niveau 1 (Recherche fine)** : ~100-150 comparaisons finales

**TOTAL : ~150 opÃ©rations au lieu de 930,000 !**

### ğŸ“ ParamÃ¨tres OptimisÃ©s

**Configuration utilisÃ©e** :

```sql
CREATE INDEX idx_documents_embedding_hnsw 
ON documents 
USING hnsw (embedding vector_cosine_ops)
WITH (
    m = 16,              -- Connexions par nÅ“ud
    ef_construction = 64 -- QualitÃ© construction
);
```

**Explication** :

- **`m = 16`** : Chaque nÅ“ud est connectÃ© Ã  16 autres nÅ“uds
  - Plus Ã©levÃ© = meilleure prÃ©cision, mais index plus gros
  - 16 = bon Ã©quilibre pour 930k docs
  
- **`ef_construction = 64`** : Taille de la file lors de la construction
  - Plus Ã©levÃ© = meilleure qualitÃ© d'index, mais construction plus lente
  - 64 = qualitÃ© optimale sans trop ralentir

- **`vector_cosine_ops`** : Utilise la distance cosinus (identique Ã  `<=>`)

### ğŸ“Š Gains de Performance Attendus

| MÃ©trique | Sans Index | Avec HNSW | Gain |
|----------|-----------|-----------|------|
| **Comparaisons** | 930,000 | ~150 | **6,200x** |
| **Temps recherche** | 30s+ (timeout) | <1s | **100x-1000x** |
| **ComplexitÃ© algo** | O(n) linÃ©aire | O(log n) logarithmique | **Exponentiel** |
| **Coverage** | 21% (200k docs) | 100% (930k docs) | **479%** |
| **UtilisabilitÃ©** | âŒ Timeout | âœ… Production | **OpÃ©rationnel** |

### â³ Construction de l'Index (13 oct 2025)

**Ã‰tat actuel** (vÃ©rification en temps rÃ©el) :

```json
{
  "index_name": "idx_documents_embedding_hnsw",
  "index_size": "199 MB",
  "is_valid": false,     // â³ PAS ENCORE PRÃŠT
  "is_ready": false,     // â³ EN CONSTRUCTION
  "estimated_rows": 930701
}
```

**PHASES DE CONSTRUCTION `CREATE INDEX CONCURRENTLY` :**

```
Phase 1: Scan initial (lecture table)        [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 40%
Phase 2: Build HNSW graph (construction)     [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 60% â† ACTUEL
Phase 3: Finalisation (optimisation)         [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80%
Phase 4: Validation (activation index)       [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%
```

**DurÃ©e par phase (930k docs Ã— 768 dims)** :
- Phase 1 : ~3-5 minutes (scan sÃ©quentiel)
- Phase 2 : ~5-10 minutes (construction graphe HNSW) â† **EN COURS**
- Phase 3 : ~2-3 minutes (optimisation connexions)
- Phase 4 : ~30 secondes (validation finale)

**TOTAL ESTIMÃ‰ : 10-20 minutes**

**Surveillance de la progression** :

```sql
-- VÃ©rifier l'Ã©tat toutes les 2-3 minutes
SELECT 
    i.relname,
    pg_size_pretty(pg_relation_size(i.oid)) as size,
    idx.indisvalid as valid,
    idx.indisready as ready
FROM pg_class i
JOIN pg_index idx ON i.oid = idx.indexrelid
WHERE i.relname = 'idx_documents_embedding_hnsw';
```

**QUAND `is_valid = true` ET `is_ready = true` :**
- âœ… Index ACTIF et utilisable
- âœ… PostgreSQL l'utilisera automatiquement
- âœ… Recherche <1s sur 930k docs

### ğŸ”§ Pourquoi l'Index N'est Pas Encore UtilisÃ© ?

**PostgreSQL refuse d'utiliser un index `CONCURRENTLY` tant qu'il n'est pas complÃ¨tement validÃ©.**

**Raison** : Garantir la cohÃ©rence des donnÃ©es
- Index invalide = rÃ©sultats potentiellement incorrects
- PostgreSQL prÃ©fÃ¨re faire une recherche linÃ©aire (lente mais correcte)
- Une fois validÃ© â†’ utilisation automatique

**CE QUI SE PASSE MAINTENANT :**

```
10:42:09 - ğŸš€ Recherche vectorielle HNSW activÃ©e (930k docs, <1s) âœ…
           â†“
10:42:09 - PostgreSQL vÃ©rifie l'index
           â†“
           Index is_valid = false âŒ
           â†“
           PostgreSQL IGNORE l'index
           â†“
           Recherche linÃ©aire sur 930k docs
           â†“
10:42:39 - TIMEOUT (30s dÃ©passÃ©s) âŒ
```

**APRÃˆS VALIDATION (is_valid = true) :**

```
Future - ğŸš€ Recherche vectorielle HNSW activÃ©e (930k docs, <1s) âœ…
           â†“
Future - PostgreSQL vÃ©rifie l'index
           â†“
           Index is_valid = true âœ…
           â†“
           PostgreSQL UTILISE l'index HNSW
           â†“
           Recherche HNSW sur ~150 vecteurs
           â†“
Future - RÃ©sultats en <1s âœ…
```

---

## âš¡ Optimisation Performance (13 oct 2025 - CRITIQUE)

### ğŸ› ProblÃ¨me IdentifiÃ©

**SymptÃ´me** : Recherche vectorielle timeout (>30s) â†’ Chatbot ne peut pas utiliser RAG

**Cause** : 930,394 documents sans index HNSW = **recherche linÃ©aire complÃ¨te**
- Chaque recherche calcule la distance cosinus pour TOUS les documents
- Temps: O(n) oÃ¹ n = 930k â†’ timeout Supabase (30s)

### âœ… Fix Temporaire (EN PRODUCTION)

**Fichier** : `Agent-Orchestrator/backend/services/supabase_search_service.py`  
**Lignes** : 261-272  
**Date** : 13 oct 2025 12:11 UTC  
**Commit** : `b704cc1`

```python
# âš¡ OPTIMISATION TEMPORAIRE (en attendant index HNSW)
# Limiter la recherche aux 200k documents les plus rÃ©cents
where_conditions.append(
    f"""id IN (
        SELECT id FROM documents 
        ORDER BY upload_date DESC NULLS LAST 
        LIMIT 200000
    )"""
)
```

**RÃ©sultats** :
- âœ… Temps : 30s+ â†’ 5-10s estimÃ©
- âœ… Coverage : 21% de la base (docs les plus rÃ©cents)
- âš ï¸ Limitation : N'accÃ¨de pas aux 730k documents plus anciens

### ğŸš€ Solution Permanente : Index HNSW

**Fichier** : `DOCS-ARCHITECTURE/CREATE_HNSW_INDEXES.sql`  
**Status** : âš ï¸ **Ã€ EXÃ‰CUTER MANUELLEMENT**

#### ExÃ©cution

1. Ouvrir SQL Editor Supabase : https://supabase.com/dashboard/project/joozqsjbcwrqyeqepnev/sql/new
2. Copier-coller le contenu de `CREATE_HNSW_INDEXES.sql`
3. Cliquer RUN (durÃ©e : 5-15 minutes)

#### Gains Attendus

| MÃ©trique | Avant (sans index) | AprÃ¨s (HNSW) | Gain |
|----------|-------------------|--------------|------|
| Temps recherche | 30s+ (timeout) | <1s | **100x-1000x** |
| Coverage | 21% (200k docs) | 100% (930k docs) | **479%** |
| ComplexitÃ© | O(n) = 930k | O(log n) | **Logarithmique** |

### ğŸ”„ AprÃ¨s CrÃ©ation Index HNSW

1. Retirer la limitation temporaire (lignes 261-272 de `supabase_search_service.py`)
2. VÃ©rifier les performances : recherche < 1s
3. Commit de nettoyage

---

## ğŸ”„ Maintenance Index HNSW (13 oct 2025)

### âœ… Mise Ã  Jour Automatique

**Question** : L'index HNSW se met-il Ã  jour automatiquement quand j'insÃ¨re de nouveaux documents avec embeddings ?

**RÃ©ponse** : **OUI, AUTOMATIQUEMENT !** âœ…

**Processus** :

```sql
-- Worker insÃ¨re nouveau document
INSERT INTO documents (title, content, embedding)
VALUES ('Nouveau Code', 'Article L152-1...', '[0.123, -0.456, ...]');
```

**PostgreSQL fait automatiquement** :

1. âœ… Insert document dans table `documents`
2. âœ… Calcule position optimale dans graphe HNSW
3. âœ… CrÃ©e connexions avec nÅ“uds proches (m=16 connexions)
4. âœ… Met Ã  jour l'index HNSW
5. âœ… Document **immÃ©diatement cherchable** via recherche vectorielle

**Temps ajoutÃ©** : +0.1-0.5s par insertion (overhead index)

**Aucune action manuelle requise !** Les nouveaux embeddings sont automatiquement indexÃ©s.

---

### âš ï¸ DÃ©gradation Progressive

**ProblÃ¨me** : AprÃ¨s des milliers d'insertions, l'index HNSW peut se fragmenter.

**SymptÃ´mes** :
- Recherche ralentit progressivement : <1s â†’ 2-3s
- QualitÃ© rÃ©sultats lÃ©gÃ¨rement dÃ©gradÃ©e
- Graphe HNSW sous-optimal

**Cause** :
- Insertions successives crÃ©ent des connexions locales sous-optimales
- Le graphe n'est plus globalement optimal
- Certains nÅ“uds sont mal connectÃ©s

**Seuil critique** : AprÃ¨s **+100,000 insertions** ou **+10% de la base**

---

### ğŸ”§ Solution : REINDEX PÃ©riodique

**Commande** :

```sql
-- Reconstruire l'index HNSW de faÃ§on optimale
REINDEX INDEX CONCURRENTLY idx_documents_embedding_hnsw;
```

**Effet** :
- âœ… Reconstruit graphe HNSW complet
- âœ… Reconnecte tous les nÅ“uds de faÃ§on optimale
- âœ… Restaure performance <1s
- âœ… AmÃ©liore qualitÃ© rÃ©sultats

**DurÃ©e** : 10-20 minutes (selon taille base)

**Downtime** : **AUCUN** (grÃ¢ce Ã  `CONCURRENTLY`)
- PostgreSQL utilise l'ancien index pendant reconstruction
- Bascule automatiquement sur le nouvel index quand prÃªt

---

### ğŸ“… Cron Job RecommandÃ©

**CrÃ©er un cron job mensuel** :

```sql
-- CrÃ©er cron job REINDEX mensuel (1er du mois Ã  3h du matin)
SELECT cron.schedule(
    'monthly-reindex-hnsw',
    '0 3 1 * *',
    $$
    -- REINDEX index HNSW documents
    REINDEX INDEX CONCURRENTLY idx_documents_embedding_hnsw;
    
    -- REINDEX index HNSW chunks (quand actif)
    REINDEX INDEX CONCURRENTLY idx_document_chunks_embedding_hnsw;
    
    -- Mettre Ã  jour statistiques
    ANALYZE documents;
    ANALYZE document_chunks;
    $$
);
```

**VÃ©rifier les cron jobs** :

```sql
SELECT * FROM cron.job WHERE jobname LIKE '%reindex%';
```

**Logs d'exÃ©cution** :

```sql
SELECT * FROM cron.job_run_details 
WHERE jobid = (SELECT jobid FROM cron.job WHERE jobname = 'monthly-reindex-hnsw')
ORDER BY start_time DESC 
LIMIT 10;
```

---

### ğŸ“Š Monitoring SantÃ© Index

**VÃ©rifier si REINDEX nÃ©cessaire** :

```sql
-- Comparer performance actuelle vs optimale
WITH test_query AS (
    SELECT '[0.1, 0.2, ...]'::vector as query_embedding
)
EXPLAIN ANALYZE
SELECT id, embedding <=> (SELECT query_embedding FROM test_query) as distance
FROM documents
WHERE embedding <=> (SELECT query_embedding FROM test_query) < 0.3
ORDER BY distance
LIMIT 8;
```

**Indicateurs** :
- âœ… Temps < 500ms : Index optimal
- âš ï¸ Temps 500ms-2s : Index OK mais peut Ãªtre optimisÃ©
- âŒ Temps > 2s : **REINDEX recommandÃ©**

**VÃ©rifier fragmentation** :

```sql
-- Nombre de scans index vs table
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched,
    CASE 
        WHEN idx_scan > 0 THEN ROUND(100.0 * idx_tup_fetch / idx_tup_read, 2)
        ELSE 0
    END as efficiency_pct
FROM pg_stat_user_indexes
WHERE indexname LIKE '%hnsw%';
```

---

### ğŸ“‹ Checklist Maintenance Index HNSW

**Mensuel** :
- [ ] REINDEX CONCURRENTLY (cron job automatique)
- [ ] ANALYZE tables
- [ ] VÃ©rifier logs cron job
- [ ] Tester temps recherche (<500ms)

**AprÃ¨s Ã©vÃ©nement majeur** :
- [ ] REINDEX si +100k insertions d'un coup
- [ ] REINDEX si changement version pgvector
- [ ] REINDEX si migration donnÃ©es

**Monitoring continu** :
- [ ] Temps recherche < 1s
- [ ] Index size stable (~200 MB pour 930k docs)
- [ ] Efficiency > 95%

---

## ğŸ“š RÃ©fÃ©rences

- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [pgvector HNSW Indexes](https://github.com/pgvector/pgvector#hnsw)
- [llama.cpp Documentation](https://github.com/ggerganov/llama.cpp)
- [Hugging Face - Solon Embeddings](https://huggingface.co/OrdalieTech/Solon-embeddings-base-0.1)

