# üß© ARCHITECTURE WORKERLOCAL CHUNK

**Date** : 15 octobre 2025  
**Version** : 2.1  
**Type** : CLI Python (Windows/Linux compatible)  
**R√¥le** : Parser documents + G√©n√©rer embeddings **GRANULAIRES** (chunks)

---

## üéØ RESPONSABILIT√âS

### **CE QU'IL FAIT** ‚úÖ

```
1. Lit files_queue (status: pending_chunk)
   ‚Üì T√©l√©charge JSON bucket
2. Parse + D√©coupe en chunks
   ‚Üì Chunking s√©mantique (500-1000 tokens)
3. G√©n√®re embedding par CHUNK (GGUF)
   ‚Üì Chunk ‚Üí 1 vecteur 768 dims
4. INSERT dans document_chunks table
   ‚Üì + Embeddings pgvector
5. UPDATE files_queue (status: processed_chunk)
   ‚Üì INSERT parsed_files_chunks (tracking)
```

**Diff√©rence avec WorkerLocal** :

| Feature | WorkerLocal | WorkerLocal Chunk |
|---------|-------------|-------------------|
| **Embedding** | 1 global (document entier) | N granulaires (chunks) |
| **Table** | `documents` | `document_chunks` |
| **Chunking** | ‚ùå Non | ‚úÖ Oui (500-1000 tokens) |
| **Use case** | Recherche doc entier | Recherche passage pr√©cis |
| **Status queue** | `processed` | `processed_chunk` |

### **CE QU'IL NE FAIT PAS** ‚ùå

- ‚ùå Collecte donn√©es (Micro-service L√©gifrance)
- ‚ùå Embeddings globaux (WorkerLocal)
- ‚ùå Chat LLM (Backend)
- ‚ùå RAG search (Backend)

---

## üìä ARCHITECTURE CHUNKING

### **Strat√©gie Chunking S√©mantique**

**Algorithme** :
1. **Split par sections** : D√©coupe selon structure juridique
2. **Taille cible** : 500-1000 tokens
3. **Overlap** : 10% entre chunks (contexte)
4. **Pr√©servation** : Garde structure logique

**Exemple** :

```
Document original (3000 tokens)
    ‚Üì
Chunk 1 (500 tokens) : Introduction + Article 1
Chunk 2 (550 tokens) : Article 2-3 (+ overlap 50 tokens)
Chunk 3 (600 tokens) : Article 4-5 (+ overlap 60 tokens)
...
```

---

## üìÅ STRUCTURE FICHIERS

```
WorkerLocal Chunk/
‚îú‚îÄ‚îÄ cli.py                      # Point d'entr√©e CLI
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py            # Configuration env
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îî‚îÄ‚îÄ supabase_client.py     # Client asyncpg
‚îú‚îÄ‚îÄ embedding/
‚îÇ   ‚îî‚îÄ‚îÄ gguf_service.py        # Service embeddings GGUF
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îî‚îÄ‚îÄ supabase_storage.py    # Download bucket
‚îú‚îÄ‚îÄ worker/
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py             # Chunking s√©mantique
‚îÇ   ‚îú‚îÄ‚îÄ parser.py              # Parsing JSON l√©gal
‚îÇ   ‚îî‚îÄ‚îÄ processor.py           # Workflow complet
‚îú‚îÄ‚îÄ launch/
‚îÇ   ‚îú‚îÄ‚îÄ worker_chunk_1.bat     # Launcher Worker Chunk 1
‚îÇ   ‚îú‚îÄ‚îÄ worker_chunk_2.bat     # Launcher Worker Chunk 2
‚îÇ   ‚îî‚îÄ‚îÄ worker_chunk_3.bat     # Launcher Worker Chunk 3
‚îú‚îÄ‚îÄ cache/                      # Cache mod√®le GGUF
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances
‚îî‚îÄ‚îÄ README.md                   # Documentation
```

---

## üîß COMPOSANTS PRINCIPAUX

### **1. DocumentChunker** (`worker/chunker.py`)

```python
class DocumentChunker:
    def __init__(self, chunk_size: int = 1000, overlap: int = 100):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def chunk_document(self, content: str, metadata: dict) -> List[dict]:
        """
        D√©coupe document en chunks s√©mantiques
        
        Args:
            content: Texte complet document
            metadata: M√©tadonn√©es document
        
        Returns:
            Liste chunks avec m√©tadonn√©es
        """
        # 1. Tokenize texte
        tokens = self.tokenizer.encode(content)
        
        # 2. Split en chunks avec overlap
        chunks = []
        i = 0
        
        while i < len(tokens):
            # Chunk tokens
            chunk_tokens = tokens[i:i + self.chunk_size]
            
            # Decode back to text
            chunk_text = self.tokenizer.decode(chunk_tokens)
            
            chunks.append({
                "content": chunk_text,
                "chunk_index": len(chunks),
                "token_count": len(chunk_tokens),
                "start_token": i,
                "metadata": {
                    **metadata,
                    "chunk_size": self.chunk_size,
                    "overlap": self.overlap
                }
            })
            
            # Avancer avec overlap
            i += self.chunk_size - self.overlap
        
        logger.info(
            "üì¶ Document chunked",
            total_tokens=len(tokens),
            num_chunks=len(chunks),
            avg_chunk_size=sum(c['token_count'] for c in chunks) / len(chunks)
        )
        
        return chunks
```

---

### **2. ChunkProcessor** (`worker/processor.py`)

```python
class ChunkProcessor:
    def __init__(self, worker_id: int, batch_size: int):
        self.worker_id = worker_id
        self.batch_size = batch_size
        self.db_client = SupabaseDBClient()
        self.storage_client = SupabaseStorageClient()
        self.embedding_service = GGUFEmbeddingService()
        self.parser = LegalDocumentParser()
        self.chunker = DocumentChunker(chunk_size=1000, overlap=100)
    
    async def process_file(self, file: dict):
        """
        Traite fichier complet : Parse + Chunk + Embed
        """
        # 1. Download JSON bucket
        content = await self.storage_client.download(
            bucket=file['bucket_name'],
            path=file['file_path']
        )
        
        # 2. Parse document
        document = self.parser.parse(content)
        
        # 3. D√©coupe en chunks
        chunks = self.chunker.chunk_document(
            content=document['content'],
            metadata=document['metadata']
        )
        
        # 4. Pour chaque chunk : Embed + INSERT
        for chunk in chunks:
            try:
                # G√©n√®re embedding chunk
                embedding = await self.embedding_service.generate(chunk['content'])
                
                # INSERT document_chunks
                await self.db_client.insert_chunk(
                    document_id=None,  # Future: lien vers documents table
                    chunk_index=chunk['chunk_index'],
                    content=chunk['content'],
                    token_count=chunk['token_count'],
                    embedding=embedding,
                    metadata=chunk['metadata'],
                    source_file=file['file_path']
                )
                
                logger.debug(
                    "‚úÖ Chunk inserted",
                    file_path=file['file_path'],
                    chunk_index=chunk['chunk_index']
                )
            
            except Exception as e:
                logger.error(
                    "‚ùå Error processing chunk",
                    file_path=file['file_path'],
                    chunk_index=chunk['chunk_index'],
                    error=str(e)
                )
        
        # 5. UPDATE files_queue status
        await self.db_client.update_file_status(
            file_id=file['id'],
            status='processed_chunk',
            worker_id=self.worker_id
        )
        
        logger.info(
            "‚úÖ File processed (chunks)",
            file_path=file['file_path'],
            num_chunks=len(chunks),
            worker_id=self.worker_id
        )
```

---

## üîÑ WORKFLOW COMPLET

```mermaid
sequenceDiagram
    participant WLC as WorkerLocal Chunk
    participant FQ as files_queue
    participant BKT as Bucket
    participant CHUNK as Chunker
    participant GGUF as GGUF Service
    participant DB as document_chunks
    
    loop Boucle infinie
        WLC->>FQ: SELECT * WHERE status=pending_chunk LIMIT 100
        FQ-->>WLC: 100 fichiers
        
        loop Pour chaque fichier
            WLC->>BKT: Download JSON
            BKT-->>WLC: JSON content
            
            WLC->>WLC: Parse document
            
            WLC->>CHUNK: chunk_document(content)
            CHUNK-->>WLC: [chunk1, chunk2, ..., chunkN]
            
            loop Pour chaque chunk
                WLC->>GGUF: generate(chunk.content)
                GGUF-->>WLC: embedding[768]
                
                WLC->>DB: INSERT document_chunks + embedding
            end
            
            WLC->>FQ: UPDATE status=processed_chunk
        end
    end
```

---

## üìä TABLES ASSOCI√âES

### **document_chunks**

```sql
CREATE TABLE document_chunks (
    id BIGSERIAL PRIMARY KEY,
    document_id BIGINT REFERENCES documents(id),  -- Future
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    token_count INTEGER,
    embedding vector(768),
    metadata JSONB,
    source_file TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index HNSW pour recherche vectorielle
CREATE INDEX idx_document_chunks_embedding_hnsw 
ON document_chunks 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 24, ef_construction = 96);
```

**Diff√©rence vs `documents`** :
- ‚úÖ Plus de chunks que de documents (1 doc ‚Üí N chunks)
- ‚úÖ Index HNSW plus gros (6M chunks vs 312k docs)
- ‚úÖ Recherche plus pr√©cise (passages vs documents entiers)

---

## ‚öôÔ∏è CONFIGURATION

### **Variables d'environnement**

**Fichier** : `.env`

```bash
# Supabase
SUPABASE_URL=https://joozqsjbcwrqyeqepnev.supabase.co
SUPABASE_SERVICE_ROLE_KEY=xxx
DATABASE_URL=postgresql://postgres.joozqsjbcwrqyeqepnev:***@aws-0-eu-central-1.pooler.supabase.com:5432/postgres

# GGUF Model
GGUF_MODEL_NAME=solon-embeddings-large-0.1-Q8_0.gguf
GGUF_BUCKET_NAME=ai-models
EMBEDDING_DIM=768

# Chunking Config
CHUNK_SIZE=1000           # Tokens par chunk
CHUNK_OVERLAP=100         # Tokens overlap
BATCH_SIZE=50             # Fichiers par batch

# Worker Config
MAX_RETRIES=3
```

---

### **Launchers Windows**

**Fichier** : `launch/worker_chunk_1.bat`

```batch
@echo off
cd /d "%~dp0.."
call venv\Scripts\activate
python cli.py --worker-id 1 --batch-size 50
pause
```

**Usage** :
```batch
# Lancer Worker Chunk 1
launch\worker_chunk_1.bat

# Lancer Worker Chunk 2
launch\worker_chunk_2.bat

# Lancer Worker Chunk 3
launch\worker_chunk_3.bat
```

---

## üìä ESTIMATIONS

### **Capacit√© Syst√®me**

**Donn√©es actuelles** :
- Documents : 312,000
- Chunks estim√©s : ~6,000,000 (ratio 1:20)
- Embedding size : 768 √ó 4 bytes = 3 KB/chunk
- Total embeddings : 6M √ó 3 KB ‚âà **18 GB**

**Index HNSW** :
- Param√®tres : m=24, ef_construction=96
- Taille estim√©e : ~2.5 GB
- Latence : <300ms
- Recall : >97%

---

## üéØ USE CASES

### **Recherche Hybride Future**

**Strat√©gie** :
1. **Recherche globale** (documents) : Vue d'ensemble
2. **Recherche granulaire** (chunks) : Passages pr√©cis
3. **Combinaison** : Meilleurs chunks + document parent

**Exemple** :
```python
# 1. Recherche chunks (pr√©cision)
chunks = await search_chunks(
    query="PLU coefficient d'emprise au sol",
    limit=10
)

# 2. R√©cup√®re documents parents (contexte)
docs = await get_parent_documents(chunk_ids=[c.id for c in chunks])

# 3. Construire contexte LLM
context = build_hybrid_context(chunks, docs)
```

---

## üîß FIX CRITIQUE

### **Embeddings Incompatibles** (m√™me que WorkerLocal)

**Solution** :
```bash
# Forcer compilation source sans AVX2/FMA
pip uninstall llama-cpp-python
pip install --no-binary=llama-cpp-python llama-cpp-python
```

**R√©sultat** :
- ‚úÖ Embeddings identiques Workers ‚Üî Backend
- ‚úÖ RAG granulaire fonctionnel

---

## üéâ R√©sum√©

**WorkerLocal Chunk optimis√©** :
- ‚úÖ CLI Python multi-workers
- ‚úÖ Chunking s√©mantique (500-1000 tokens)
- ‚úÖ Embeddings GRANULAIRES GGUF (768 dims)
- ‚úÖ 6M chunks estim√©s (1:20 ratio)
- ‚úÖ Recherche ultra-pr√©cise
- ‚úÖ Pr√™t pour RAG hybride
- ‚úÖ Compatible Windows/Linux

**Chunking granulaire au top !** üß©

