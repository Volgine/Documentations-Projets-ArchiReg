# üìã PLAN COMPLET : ARCHITECTURE RAG & EMBEDDINGS

**Date** : 2025-10-11  
**Version** : 1.0 D√âFINITIVE  
**Status** : ‚úÖ ARCHITECTURE CLARIFI√âE

---

## üéØ ARCHITECTURE GLOBALE (100% CLAIRE)

```mermaid
graph TB
    subgraph "SOURCE: COLLECTE DONN√âES"
        A[Micro-service L√©gifrance PISTE] -->|Upload JSON bruts| B[Bucket Supabase Storage<br/>agentbasic-legifrance-raw]
    end
    
    subgraph "TRAITEMENT: WORKERS LOCAUX"
        B -->|R√©cup√®re fichiers| C[Worker Local x3<br/>ACTUEL]
        C -->|Parse + GGUF Embedding| D[(Table documents<br/>1 doc = 1 embedding contexte)]
        D -->|Vector embeddings| E[pgvector extension]
        
        H[Worker Local FUTUR<br/>Chunking Granulaire] -.->|Parse + Chunks| I[(Table document_chunks<br/>Embeddings granulaires)]
        B -.->|Source future| H
        I -.->|Vector embeddings| E
    end
    
    subgraph "BACKEND: RECHERCHE S√âMANTIQUE"
        F[Backend Agent-Orchestrator] -->|READ ONLY| E
        F -->|G√©n√®re embedding query| K[GGUF Model Local<br/>Solon-embeddings-base-0.1.Q8_0.gguf]
        K -->|T√©l√©charg√© depuis| L[Bucket Supabase<br/>ai-models]
        F -->|Recherche vectorielle| E
        F -->|API RAG /api/v3/rag/search-legifrance| G[Frontend Chat]
    end
    
    style C fill:#4ecdc4
    style F fill:#ff6b6b
    style H fill:#95e1d3,stroke-dasharray: 5 5
    style I fill:#95e1d3,stroke-dasharray: 5 5
    style K fill:#ffd93d
    style L fill:#ffd93d
```

---

## üìä RESPONSABILIT√âS PAR SERVICE

### 1Ô∏è‚É£ **Micro-service L√©gifrance** (@Micro-service-data-legifrance-piste/)
**R√¥le** : Collecter les donn√©es juridiques depuis API PISTE  
**Actions** :
- ‚úÖ Appel API PISTE (codes, textes juridiques)
- ‚úÖ T√©l√©chargement fichiers JSON bruts
- ‚úÖ Upload dans bucket Supabase `agentbasic-legifrance-raw`
- ‚úÖ Alimentation continue des donn√©es

**Output** : Fichiers JSON dans `agentbasic-legifrance-raw/legifrance/architecture_urbanisme/`

---

### 2Ô∏è‚É£ **Worker Local** (@WorkerLocal/) üè≠

#### **ACTUEL (v1.0 - Embeddings Contexte Global)**
**R√¥le** : Parser les documents et g√©n√©rer les embeddings de contexte global  
**Actions** :
1. ‚úÖ R√©cup√®re fichiers JSON depuis bucket L√©gifrance
2. ‚úÖ Parse le JSON (extraction texte : titre, contenu, articles, sections)
3. ‚úÖ G√©n√®re 1 embedding GGUF par document complet (contexte global)
4. ‚úÖ INSERT dans table `documents` avec colonne `embedding vector(768)`
5. ‚úÖ Anti-duplication via `parsed_files` (content_hash)

**Mod√®le utilis√©** :
- `Solon-embeddings-base-0.1.Q8_0.gguf` (768 dimensions)
- Stock√© localement dans `WorkerLocal/models/`
- Chargement via `llama-cpp-python`

**Configuration** :
- Batch size : 100 fichiers
- Concurrence : 50 threads
- Timeout download : 30s
- Timeout processing : 30s

**Output** : Table `documents` avec embeddings contexte global

#### **FUTUR (v2.0 - Chunking Granulaire)** üîÆ
**R√¥le** : D√©couper les documents en chunks et g√©n√©rer des embeddings granulaires  
**Actions** :
1. ‚è∏Ô∏è R√©cup√®re fichiers JSON depuis bucket L√©gifrance
2. ‚è∏Ô∏è Parse le JSON en chunks (articles, sections, paragraphes)
3. ‚è∏Ô∏è G√©n√®re 1 embedding GGUF par chunk (recherche pr√©cise)
4. ‚è∏Ô∏è INSERT dans table `document_chunks` avec colonne `embedding vector(768)`
5. ‚è∏Ô∏è Lien avec document parent via `document_id`

**Mod√®le utilis√©** : M√™me mod√®le GGUF (Solon-embeddings-base-0.1.Q8_0.gguf)

**Output** : Table `document_chunks` avec embeddings granulaires (FUTUR)

---

### 3Ô∏è‚É£ **Backend Agent-Orchestrator** (@Agent-Orchestrator/) üî¥

**R√¥le** : Chatbot + Recherche s√©mantique **READ ONLY**  
**Actions** :
1. ‚úÖ T√©l√©charge le mod√®le GGUF depuis bucket `ai-models` au d√©marrage (lazy loading)
2. ‚úÖ Charge le mod√®le en m√©moire via subprocess `llama-cpp-python`
3. ‚úÖ Re√ßoit une query utilisateur (ex: "r√®gles d'urbanisme")
4. ‚úÖ G√©n√®re l'embedding de la query via `GGUFEmbeddingService`
5. ‚úÖ Effectue une recherche vectorielle dans pgvector :
   ```sql
   SELECT id, title, content, embedding <-> query_vector AS distance
   FROM documents
   WHERE embedding <-> query_vector < 0.3
   ORDER BY distance
   LIMIT 10;
   ```
6. ‚úÖ Retourne les r√©sultats au frontend

**Endpoints API** :
- `GET /api/v3/rag/search-legifrance?query=...&limit=10`
- `GET /api/v3/rag/legifrance-stats`
- `GET /api/v3/rag/documents/top?projectId=...&k=5`

**Services cl√©s** :
- `GGUFEmbeddingService` : G√©n√®re embeddings pour les queries
- `SupabaseSearchService` : Recherche vectorielle pgvector
- `RAGService` : Orchestration RAG compl√®te

**‚ö†Ô∏è IMPORTANT** :
- ‚ùå **PAS** de g√©n√©ration d'embeddings pour les documents (c'est le r√¥le des workers)
- ‚ùå **PAS** de parsing de fichiers
- ‚ùå **PAS** d'√©criture dans `documents` ou `document_chunks`
- ‚úÖ **SEULEMENT** recherche s√©mantique READ ONLY

---

### 4Ô∏è‚É£ **Frontend** (@ArchiReg-Front/) üåê

**R√¥le** : Interface utilisateur pour le chat et la recherche  
**Actions** :
- ‚úÖ Affiche l'interface chat
- ‚úÖ Envoie la query utilisateur au backend
- ‚úÖ Affiche les r√©sultats de recherche s√©mantique
- ‚úÖ G√®re l'authentification utilisateur (JWT)

---

## üóÑÔ∏è STRUCTURE BASE DE DONN√âES

### **Table `documents` (ACTUELLE)**
```sql
CREATE TABLE documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  content TEXT NOT NULL,
  embedding VECTOR(768) NOT NULL, -- ‚úÖ G√©n√©r√© par Workers
  file_path TEXT NOT NULL UNIQUE,
  upload_date TIMESTAMPTZ DEFAULT NOW(),
  extra_data JSONB
);

-- Index pgvector pour recherche rapide
CREATE INDEX idx_documents_embedding ON documents 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);
```

**Remplissage** : Workers Locaux (1 doc = 1 embedding contexte)  
**Lecture** : Backend Agent-Orchestrator (recherche s√©mantique)

---

### **Table `document_chunks` (FUTURE)**
```sql
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
  chunk_text TEXT NOT NULL,
  chunk_index INT NOT NULL,
  embedding VECTOR(768) NOT NULL, -- ‚úÖ G√©n√©r√© par Workers (futur)
  metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index pgvector pour recherche granulaire
CREATE INDEX idx_chunks_embedding ON document_chunks 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);
```

**Remplissage** : Workers Locaux (FUTUR - chunking granulaire)  
**Lecture** : Backend Agent-Orchestrator (recherche pr√©cise)

---

### **Table `parsed_files` (Anti-duplication)**
```sql
CREATE TABLE parsed_files (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_path TEXT NOT NULL UNIQUE,
  content_hash TEXT NOT NULL,
  worker_id TEXT NOT NULL,
  status TEXT DEFAULT 'completed',
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**R√¥le** : √âviter de reparser les m√™mes fichiers

---

## üß† MOD√àLE GGUF : SOLON-EMBEDDINGS-BASE

### **Caract√©ristiques**
- **Nom** : `Solon-embeddings-base-0.1.Q8_0.gguf`
- **Dimensions** : 768
- **Taille** : ~500 MB
- **Quantization** : Q8_0 (optimis√© performance/qualit√©)
- **Langue** : Fran√ßais (optimis√© pour textes juridiques)

### **Stockage**
- **Bucket Supabase** : `ai-models/Solon-embeddings-base-0.1.Q8_0.gguf`
- **URL publique** : `https://joozqsjbcwrqyeqepnev.supabase.co/storage/v1/object/public/ai-models/Solon-embeddings-base-0.1.Q8_0.gguf`
- **Cache local Backend** : `Agent-Orchestrator/backend/cache/shared/`
- **Cache local Worker** : `WorkerLocal/models/`

### **Chargement**
**Backend** (lazy loading) :
```python
# services/supabase_embedding_service.py
async def _download_model_if_needed(self):
    if not self.local_cache_path.exists():
        async with aiohttp.ClientSession() as session:
            async with session.get(self.supabase_storage_url) as response:
                with open(self.local_cache_path, "wb") as f:
                    async for chunk in response.content.iter_chunked(8192):
                        f.write(chunk)
```

**Worker** (au d√©marrage) :
```python
# embedding/llama_service.py
async def _load_model(self):
    self.llm = Llama(
        model_path=str(self.model_path),
        embedding=True,
        n_ctx=4096,
        n_batch=2048,
        n_threads=cpu_count,
        n_ubatch=2048,
        low_vram=False,
        f16_kv=True,
        verbose=False
    )
```

---

## üîÑ FLUX DE DONN√âES COMPLET

### **Phase 1 : Collecte** üì°
```
Micro-service L√©gifrance PISTE
    ‚Üì API PISTE
Fichiers JSON bruts
    ‚Üì Upload
Bucket Supabase Storage (agentbasic-legifrance-raw)
```

### **Phase 2 : Traitement** üè≠
```
Bucket Storage
    ‚Üì T√©l√©chargement
Worker Local (batch 100 fichiers)
    ‚Üì Parse JSON
Extraction texte (titre + contenu + articles)
    ‚Üì G√©n√©ration embedding GGUF (768 dims)
Embedding contexte global
    ‚Üì INSERT
Table documents (avec colonne embedding)
    ‚Üì Indexation
pgvector (ivfflat index)
```

### **Phase 3 : Recherche** üîç
```
Frontend (query utilisateur)
    ‚Üì HTTP POST
Backend /api/v3/rag/search-legifrance
    ‚Üì G√©n√©ration embedding query
GGUFEmbeddingService (mod√®le local)
    ‚Üì Embedding query (768 dims)
Recherche vectorielle pgvector
    ‚Üì Distance cosinus
SELECT ... ORDER BY embedding <-> query_vector
    ‚Üì Top K r√©sultats
Retour au Frontend
```

---

## üö® PROBL√àME ACTUEL : TEST RAG

### **Sympt√¥me**
```
‚ùå RAG KO: Failed to generate embeddings: {"code":"NOT_FOUND","message":"Requested function...
```

### **Cause Identifi√©e**
L'Edge Function `rag-endpoint` **N'EXISTE PAS** !

Le test `test-rag` dans `system-tests` Edge Function essaie d'appeler :
```typescript
fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/rag-endpoint`)
```

Mais cette Edge Function n'a jamais √©t√© cr√©√©e !

### **Solution**
**Option 1 : Supprimer l'Edge Function inexistante** ‚úÖ RECOMMAND√â
- Modifier le test `test-rag` pour appeler directement le backend :
  ```typescript
  fetch(`${BACKEND_URL}/api/v3/rag/search-legifrance?query=test+urbanisme&limit=3`)
  ```
- Le backend poss√®de d√©j√† tous les services n√©cessaires

**Option 2 : Cr√©er l'Edge Function `rag-endpoint`** ‚ùå PAS RECOMMAND√â
- Dupliquerait la logique d√©j√† pr√©sente dans le backend
- N√©cessiterait de recr√©er le service GGUF en Deno (complexe)
- Perte de performance (appels externes vs. service local)

---

## ‚úÖ PLAN D'ACTION (TERMIN√â)

### **√âtape 1 : Corriger le test RAG** üîß ‚úÖ FAIT
1. ‚úÖ Modifi√© `DOCS-ARCHITECTURE/05-EDGE-FUNCTIONS/system-tests/index.ts`
2. ‚úÖ Remplac√© l'appel √† `/functions/v1/rag-endpoint` par `/api/v3/rag/search-legifrance`
3. ‚úÖ JWT admin pass√© dans l'Authorization header
4. ‚úÖ Test recherche s√©mantique directement sur le backend

### **√âtape 2 : Mettre √† jour la documentation** üìö ‚úÖ FAIT
1. ‚úÖ Cr√©√© `DOCS-ARCHITECTURE/09-RAG-EMBEDDINGS.md` (documentation compl√®te)
2. ‚úÖ Mis √† jour `DOCS-ARCHITECTURE/00-INDEX.md` (ajout section RAG)
3. ‚úÖ Mis √† jour `DOCS-ARCHITECTURE/README.md` (ajout RAG au r√©sum√©)
4. ‚úÖ Clarifi√© que `test-rag` appelle le backend directement

### **√âtape 3 : Tester le syst√®me complet** üß™ ‚è∏Ô∏è √Ä FAIRE PAR L'UTILISATEUR
1. ‚è∏Ô∏è D√©ployer Edge Function `system-tests` (avec fix RAG)
2. ‚è∏Ô∏è Lancer `test-rag` depuis le dashboard admin
3. ‚è∏Ô∏è V√©rifier que le backend g√©n√®re l'embedding de la query
4. ‚è∏Ô∏è V√©rifier la recherche vectorielle dans pgvector
5. ‚è∏Ô∏è V√©rifier le retour des r√©sultats au frontend

---

## üìà M√âTRIQUES & PERFORMANCE

### **Workers Locaux**
- **Vitesse traitement** : 50 fichiers/sec (concurrence 50)
- **Embedding time** : ~20ms par document
- **Batch size** : 100 fichiers
- **Anti-duplication** : 99.9% efficacit√©

### **Backend RAG**
- **Embedding query** : ~15ms
- **Recherche pgvector** : ~5ms pour 10 r√©sultats
- **Latence totale** : ~20-30ms
- **Cache hits** : 60-70% (embeddings)

### **Base de donn√©es**
- **Documents index√©s** : ~650,000 (actuels)
- **Taille embeddings** : ~650k √ó 768 √ó 4 bytes = ~2 GB
- **Index pgvector** : IVFFlat (100 listes)
- **Recherche distance** : Cosinus (<0.3 = pertinent)

---

## üîÆ √âVOLUTION FUTURE (v2.0)

### **Chunking Granulaire**
1. ‚è∏Ô∏è Worker Local v2 : D√©coupage en chunks
2. ‚è∏Ô∏è Table `document_chunks` avec embeddings granulaires
3. ‚è∏Ô∏è Recherche hybride : contexte global + chunks pr√©cis
4. ‚è∏Ô∏è Am√©lioration pr√©cision recherche (articles sp√©cifiques)

### **Am√©lioration Mod√®le**
1. ‚è∏Ô∏è Test Solon-embeddings-large (1024 dims)
2. ‚è∏Ô∏è Fine-tuning sur corpus juridique fran√ßais
3. ‚è∏Ô∏è √âvaluation benchmarks (accuracy, recall)

---

## üìù NOTES IMPORTANTES

### **S√©paration des Responsabilit√©s**
- ‚úÖ **Workers** = WRITE (g√©n√©ration embeddings + INSERT documents)
- ‚úÖ **Backend** = READ (g√©n√©ration embedding query + recherche vectorielle)
- ‚ùå **Backend ne g√©n√®re PAS les embeddings des documents**

### **Source de Donn√©es**
- ‚úÖ **Bucket L√©gifrance** = Source unique pour Workers ET Future Chunking
- ‚úÖ **Pas de duplication de source**
- ‚úÖ **Anti-duplication via `parsed_files`**

### **Mod√®le GGUF**
- ‚úÖ **1 seul mod√®le** : Solon-embeddings-base-0.1.Q8_0.gguf
- ‚úÖ **Partag√©** entre Workers et Backend
- ‚úÖ **Stock√©** dans bucket Supabase `ai-models`

---

**FIN DU PLAN** üéØ

