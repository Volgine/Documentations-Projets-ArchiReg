# üî• FIX LEGIARTI - Qualit√© Collecte L√©gifrance

**Date** : 15 octobre 2025  
**Commit** : `6434c93` (Micro-service)  
**Impact** : +50% qualit√© documents, -90% documents vides, -645 lignes code mort  
**Status** : ‚úÖ **EN PRODUCTION**

---

## üö® PROBL√àME D√âCOUVERT

### Sympt√¥mes
- **10,000 documents** collect√©s mais **90% < 300 chars** apr√®s nettoyage HTML
- Contenu : Titres vides ("Partie l√©gislative", "Annexes", "Livre Ier")
- WorkerLocal g√©n√©rait des embeddings sur du contenu vide
- Base RAG pollu√©e avec r√©sultats non pertinents

### Cause Racine
L'API `/consult/legi/tableMatieres` retourne 2 types d'√©l√©ments :

```json
{
  "sections": [
    {
      "cid": "LEGISCTA000006107992",  // ‚ùå Section structurelle SANS texte
      "num": null,
      "title": "Partie l√©gislative",
      "articles": [...]
    },
    {
      "cid": "LEGIARTI000006815075",  // ‚úÖ Article r√©el AVEC texteHtml
      "num": "L110-1",
      "title": "Article L110-1"
    }
  ]
}
```

**Probl√®me** : Le micro-service extrayait **TOUS les `cid`** sans distinction !

---

## ‚úÖ SOLUTION IMPL√âMENT√âE

### 1. Filtre LEGIARTI dans `extract_article_ids_recursive()`

**Fichier** : `Micro-service-data-legifrance-piste/app/services/legifrance_service.py`

```python
def extract_article_ids_recursive(self, articles: List[Dict], _first_call=True) -> List[str]:
    """
    Extrait r√©cursivement SEULEMENT les vrais articles (LEGIARTI)
    Ignore les sections structurelles vides (LEGISCTA)
    """
    article_ids = []
    
    for article in articles:
        article_id = article.get("cid") or article.get("id")
        
        # ‚úÖ FILTRE LEGIARTI : Garder SEULEMENT vrais articles
        if article_id and article_id.startswith("LEGIARTI"):
            article_ids.append(article_id)
            logger.debug("‚úÖ Article extrait", id=article_id, num=article.get("num"))
        
        elif article_id and article_id.startswith("LEGISCTA"):
            # Section vide ignor√©e
            logger.debug("‚è≠Ô∏è Section ignor√©e (pas de texte)", 
                        id=article_id, 
                        title=article.get("title"))
        
        # R√©cursion sur les enfants
        if "articles" in article and isinstance(article["articles"], list):
            child_ids = self.extract_article_ids_recursive(article["articles"], _first_call=False)
            article_ids.extend(child_ids)
    
    return article_ids
```

### 2. Filtre Qualit√© dans `_run_collection_job()`

**Fichier** : `Micro-service-data-legifrance-piste/app/scheduler/collector_scheduler.py`

```python
# Nettoyer HTML pour mesurer le texte r√©el
texte_clean = re.sub(r'<[^>]+>', ' ', texte_html)
texte_clean = texte_clean.replace('&nbsp;', ' ').replace('&amp;', '&')
texte_clean = re.sub(r'\s+', ' ', texte_clean).strip()

# ‚úÖ FILTRE QUALIT√â : Texte > 200 chars apr√®s nettoyage
if texte_clean and len(texte_clean) >= 200:
    await self.supabase_service.save_legal_document(full_article, "architecture_urbanisme")
    documents_saved += 1
    logger.debug("‚úÖ Article sauvegard√©", clean_length=len(texte_clean))
else:
    logger.info("‚è≠Ô∏è Article ignor√© (texte < 200 chars)", 
               clean_length=len(texte_clean),
               html_length=len(texte_html))
```

### 3. Fix Parsing dans Workers

**WorkerLocal & WorkerLocal Chunk** : Gestion du wrapper `article` de `/consult/getArticle`

```python
if 'legifrance_data' in data:
    legifrance = data['legifrance_data']
    
    # ‚úÖ FIX : G√©rer wrapper "article" de /consult/getArticle
    # API retourne : {"article": {"texteHtml": "...", "num": "..."}}
    if 'article' in legifrance and isinstance(legifrance['article'], dict):
        article_data = legifrance['article']
        
        # Extraire num√©ro article
        if 'num' in article_data and article_data['num']:
            text_parts.append(f"Article {article_data['num']}")
        
        # Extraire et nettoyer texteHtml
        if 'texteHtml' in article_data:
            clean_text = self._clean_html(article_data['texteHtml'])
            if clean_text:
                text_parts.append(clean_text)
```

---

## üîÑ UNIFICATION MAINTENANCE = MASSIVE

### Avant Fix ‚ùå
- **MAINTENANCE** : `/legi/tableMatieres` + LEGIARTI + qualit√© 200 chars
- **MASSIVE** : Recherche mots-cl√©s (85+) **SANS filtre LEGIARTI** ‚Üí m√©tadonn√©es vides

**Incoh√©rence** : MASSIVE cr√©ait la DB avec docs vides, MAINTENANCE la maintenait avec docs qualit√© !

### Apr√®s Fix ‚úÖ
**M√äME STRAT√âGIE** pour les 2 modes :
1. `/consult/legi/tableMatieres` (hi√©rarchie compl√®te codes)
2. `extract_article_ids_recursive()` avec **filtre LEGIARTI**
3. `/consult/getArticle` pour chaque ID
4. **Filtre qualit√©** : texte > 200 chars apr√®s nettoyage HTML

**Diff√©rence** :
- **MAINTENANCE** : 5 codes prioritaires urbanisme, CRON 2h
- **MASSIVE** : 20 codes complets, Interval 10 min (collecte continue)

---

## üßπ NETTOYAGE CODE

### Suppressions (Total : -645 lignes)

**`legifrance_service.py`** : 1031 ‚Üí 526 lignes (-505 lignes)
- ‚úÖ `massive_architecture_siphon()` + 6 fonctions priv√©es (325 lignes)
- ‚úÖ `list_administrative_docs()` (27 lignes)
- ‚úÖ `search_urbanisme_documents()` (56 lignes)
- ‚úÖ `list_all_codes()` (20 lignes)
- ‚úÖ `search_architecture_transversal()` (62 lignes)
- ‚úÖ `get_table_matieres()` **DOUBLON** (19 lignes)

**`collector_scheduler.py`** : 582 ‚Üí 442 lignes (-140 lignes)
- ‚úÖ `_continuous_massive_siphon()` (71 lignes)
- ‚úÖ `_quota_monitoring_job()` (34 lignes)
- ‚úÖ `pause_massive_siphon()`, `resume_massive_siphon()` (24 lignes)
- ‚úÖ `is_massive_siphon_running()` (5 lignes)
- ‚úÖ Simplification `start()` et `stop()` (6 lignes)

### Commits
```bash
46e15bf - REFACTOR SCHEDULER: Unifier MAINTENANCE et MASSIVE
d378f7c - CLEAN SCHEDULER: Supprimer 5 fonctions obsol√®tes
e9184d3 - CLEAN LEGIFRANCE_SERVICE: Supprimer massive_architecture_siphon + 6 fonctions
3a13223 - CLEAN: Supprimer import os + doublon get_table_matieres + 4 fonctions test
6434c93 - FIX CRITIQUE extract_article_ids_recursive: Filtrer LEGISCTA
```

---

## üìä R√âSULTATS

### Avant Fix ‚ùå
- **10,000 documents** collect√©s
- **90% < 300 chars** (titres vides, sections structurelles)
- **10% > 3K chars** (contenu r√©el)
- WorkerLocal traitait 90% de documents inutiles

### Apr√®s Fix ‚úÖ
- **28 documents** collect√©s (filtre qualit√© strict)
- **0% < 200 chars** (filtre micro-service)
- **60% > 3K chars** (17/28 docs)
- **13 docs = 10K chars** (limite troncature)

**Distribution qualit√©** :
```
= 10K chars (PARFAIT)    : 13 docs ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
5-10K chars (BON)        :  3 docs ‚ñà‚ñà‚ñà
3-5K chars               :  1 doc  ‚ñà
1-3K chars               :  3 docs ‚ñà‚ñà‚ñà
< 1000 chars             :  8 docs ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
```

### Exemples Documents Collect√©s
1. **PRINCIPES DE L'OCDE DE BONNES PRATIQUES** (10K chars)
2. **NOMENCLATURE DES INSTALLATIONS CLASS√âES** (10K chars)
3. **LISTE DE D√âCHETS** (10K chars)
4. **CAT√âGORIES D'AM√âNAGEMENTS enqu√™te publique** (10K chars)
5. **R√âPARTITION D√âPARTEMENTS zones sismicit√©** (10K chars)

---

## üîÑ WORKERS ALIGN√âS

### Fix Article Wrapper

**Probl√®me** : `/consult/getArticle` retourne `{"article": {"texteHtml": "..."}}`  
Workers cherchaient `legifrance_data.texteHtml` au lieu de `legifrance_data.article.texteHtml`

**Solution** :
- ‚úÖ WorkerLocal : Commit `ff008a8` - Fix article wrapper + fonction `_clean_html()`
- ‚úÖ WorkerLocal Chunk : Commit `fd25b43` - Fix article wrapper + nettoyage HTML inline

**Impact** : Extraction correcte du contenu complet des articles

---

## üìà M√âTRIQUES FINALES

### Base de donn√©es
- **Bucket** : 28 fichiers (1.8 MB)
- **Documents** : 28 documents (161 KB texte)
- **Qualit√©** : **17/28 docs > 3K chars (60%)**
- **Parsed** : 28 fichiers trait√©s ‚úÖ

### Micro-service
- **Status** : ‚úÖ LIVE (deploy `3a13223`)
- **Mode** : MAINTENANCE (5 codes, CRON 2h)
- **Collecte** : 1 run r√©ussi, 28 docs, 0 erreurs
- **Quotas** : 62/280,000 (0.02%) ‚úÖ
- **Code** : 968 lignes (vs 1613 avant)

### Workers
- **WorkerLocal** : ‚úÖ Align√© (commit `ff008a8`)
- **WorkerLocal Chunk** : ‚úÖ Align√© (commit `fd25b43`)

---

## üéØ ENDPOINTS UNIFI√âS

### Modes Disponibles
```http
POST /siphonnage/mode
Body: {"mode": "MAINTENANCE"}  # 5 codes, CRON 2h
Body: {"mode": "MASSIVE"}      # 20 codes, Interval 10min

POST /aspirage/start   # D√©marre collecte (mode actuel)
POST /aspirage/stop    # Arr√™te collecte
GET  /aspirage/status  # √âtat scheduler
GET  /stats            # Statistiques compl√®tes
```

### Endpoints Supprim√©s
- ‚ùå `/aspirage/urbanisme` - Redondant (int√©gr√© dans MAINTENANCE/MASSIVE)

---

## üöÄ PROCHAINES √âTAPES

1. **Tester la recherche RAG** avec les nouveaux documents de qualit√©
2. **Lancer WorkerLocal Chunk** pour g√©n√©rer chunks granulaires
3. **Activer mode MASSIVE** pour collecter les 20 codes (10-50K articles)
4. **Monitoring continu** de la qualit√© des documents collect√©s

---

## üìö R√âF√âRENCES

- **Micro-service README** : `Micro-service-data-legifrance-piste/README.md`
- **Doc API PISTE** : `Micro-service-data-legifrance-piste/docs/docs-officiel-legifrance/`
- **WorkerLocal README** : `WorkerLocal/README.md`
- **WorkerLocal Chunk README** : `WorkerLocal Chunk/README.md`

---

**FIN DU DOCUMENT**

